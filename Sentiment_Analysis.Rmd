---
title: "Prediction in Sentiment Analysis - Amazon Sample"
subtitle: "Final Report"
author: "Philippe Lambot"
date: "April 8, 2020"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r Setup, include = FALSE}

# In the YAML, I have asked a TOC (table of contents). 
# I have also chosen to produce an html_document. 

# In the opts_chunk just below, I have chosen options to avoid messages and warnings in Sentiment_Analysis_Amazon_Final_Report.html. Messages and warnings produced by the code have already been dealt with.  
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# The next opts_chunk regulates figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# The next instruction facilitates table layout in HTML.
options(knitr.table.format = "html")

# After the present chunk, there are 13 lines of code to further regulate layout:
# - the 1st block prevents bullets appearing in the TOC (Table of Contents);
# - the 2nd block determines font size in body text parts;
# - the 3rd block generates text justification.

# Last about layout, I use the string <br> to generate empty lines.
```

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<font size="3">

<style>
body {
text-align: justify}
</style>

## <br>
## <br>
## <br>

## *********************************************************************************

## PREDICTION IN SENTIMENT ANALYSIS - AMAZON SAMPLE

## ********************************************************************************* 

<br>

## EXECUTIVE SUMMARY

<br>

In this project, **sentiment analysis** has been conducted on produced an Amazon data sample provided in UCI Machine Learning Repository. Predictive accuracy of 88 % has been reached on the validation set, against 50 % with the baseline method. 

First, **Natural Language Processing** has been performed: corpus, lowercasing, punctuation handling, stopword removal, stemming, tokenization from sentences into words and bag of words. With NLP, accuracy has gained 25 percentage points.

Second, **text mining** has brought additional accuracy improvement with 10 percentage points. Two insights have been determinant: in decision trees tokens conveying subjective information predominate; but other pieces of subjective information are not used in numerous false negatives and false positives. Such ignored subjective information has been retrieved from random samples of false negatives and false positives, exclusively on the training set; customized lists have been established with tokens having either positive or negative sentiment orientation; occurrences of these tokens in reviews have been replaced either with a positive or a negative generic token.

Third, **machine learning optimization** has boosted accuracy with 4 additional percentage points. Testing has been conducted on accuracy distributions across bootstrapped resamples. eXtreme Gradient Boosting has emerged as the most performing model in this project. 

<br>

TAGS: sentiment analysis, natural language processing, text mining, subjective information, tokenization, bag of words, word frequency, wordcloud, decision trees, false negatives, false positives, text classification, polarization, lists of positive n-grams, lists of negative n-grams, text substitution, machine learning, binary classification, eXtreme Gradient Boosting, Monotone Multi-Layer Perceptron Neural Network, Random Forest, Stochastic Gradient Boosting, Support Vector Machines with Radial Basis Function Kernel, AdaBoost Classification Trees, bootstrapping, accuracy distribution across resamples, R

GitHub: https://github.com/Dev-P-L/Sentiment-Analysis

 

<br>

TAGS: sentiment analysis, natural language processing, text mining, subjective information, tokenization, bag of words, word frequency, wordcloud, decision trees, false negatives, false positives, text classification, polarization, lists of positive n-grams, lists of negative n-grams, text substitution, machine learning, binary classification,  eXtreme Gradient Boosting, Monotone Multi-Layer Perceptron Neural Network, Random Forest, Stochastic Gradient Boosting, Support Vector Machines with Radial Basis Function Kernel, AdaBoost Classification Trees, bootstrapping, accuracy distribution across resamples, R

<br>

GitHub: https://github.com/Dev-P-L/Sentiment-Analysis

<br>

## FOREWORD TO READERS

<br>

Dear Readers, you are most welcome to run the project on your own computer if you so wish.

This project is lodged with the GitHub repository https://github.com/Dev-P-L/Sentiment-Analysis.

It is comprised of eleven files. All code is included in hmeq_Script.Rmd. It does not show in hmeq_Final_Report.html. 

For your convenience, the dataset has already been downloaded onto my GitHub repository 
https://github.com/Dev-P-L/Home-Equity-Loan-Default-Prediction wherefrom it will be automatically retrieved by hmeq_Script.Rmd code. If you so wish, you can also
easily retrieve the dataset from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences and adapt hmeq_Script.Rmd code accordingly.

You can knit hmeq_Script.Rmd (please in HTML) and produce hmeq_Final_Report.html on your own computer. On my laptop, running hmeq_Script.Rmd takes approximately four hours. For information, here are some characteristics of my environment:

 - R version 3.5.1 (2018-07-02) -- "Feather Spray",
 - RStudio Version 1.1.456 - ? 2009-2018,
 - Windows 10.

Some packages are required by hmeq_Script.Rmd. The code from hmeq_Script.Rmd contains instructions to download these packages if they are not available yet. 

<br>

```{r Cleaning up workspace and downloading packages}
# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
if(!require(SnowballC)) install.packages("SnowballC", repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
if(!require(fastAdaboost)) install.packages("fastAdaboost", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(monmlp)) install.packages("monmlp", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(utf8)) install.packages("utf8", repos = "http://cran.us.r-project.org")

# Requiring libraries.
library(tidyverse)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)
library(kernlab)
library(fastAdaboost)
library(randomForest)
library(gbm)
library(xgboost)
library(monmlp)
library(kableExtra)
library(gridExtra)
library(utf8)
```

Let's get in touch with data. 

<br>

## I. GETTING IN TOUCH WITH DATA

<br>

According to the UCI website, data are organized in a CSV file in two columns: in the first column, there are single sentences that are reviews of products; in the second column, there is a positive or negative evaluation. The ratio of positive evaluations is 50 %.

Actually, there are three files. The first will be downloaded. It is a sample of 1,000 Amazon reviews. 

That file will be split into a training set, with two thirds of reviews, and a validation set. Let's have a first quick look at a few reviews from the training set.

<br>

```{r Dowloading data}
myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/amazon_cells_labelled.txt"
reviews <- read.delim(myfile, header = FALSE, sep = "\t", quote = "", 
                      stringsAsFactors = FALSE)
rm(myfile)

reviews <- reviews %>% 
  `colnames<-`(c("text", "sentiment")) %>%
  mutate(sentiment = as.factor(gsub("1", " Pos", 
         gsub("0", "Neg", sentiment)))) %>% as.data.frame()

# Creating training index and validation index.
set.seed(1)
ind_train <- createDataPartition(y = reviews$sentiment, 
                                 times = 1, p = 2/3, list = FALSE)
ind_val <- as.integer(setdiff(1:nrow(reviews), ind_train))

# ind_train allows to select the reviews that will be used for training, 
# be it in natural language processing, in text mining or in 
# machine learning.

# For presentation in a table, selecting a few reviews at random 
# from the training reviews. First retrieving training reviews.  
df <- reviews[ind_train, ] %>% as.data.frame() %>% 
      `rownames<-`(ind_train) %>% mutate(ro = rownames(.))

# Then sampling with indication of the row numbers from the whole dataset. 
set.seed(1)
seq <- createDataPartition(y = df$sentiment, 
       times = 1, p = 12/length(ind_train), list = FALSE)
sampl <- df[seq, ] %>% as.data.frame() %>% 
         select(ro, text, sentiment) %>% `rownames<-`(NULL)
str(sampl)

# Building presentation table.
tab <- sampl %>% as.data.frame() %>% 
  `colnames<-`(c("", "TRAINING REVIEW", "SENTIMENT"))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2:3, bold = T, color = "#08457E", background = "#9bc4e2")
rm(df, seq, sampl, tab)
```

<br>

After shaking hands with data, let's proceed to some NLP.

<br>

## II. NATURAL LANGUAGE PROCESSING

### A. Corpus - Tokenization - Bag of Words

<br>
 
After data being downloaded, they will be reorganized into a corpus. Then they will be processed in NLP: words will be lowercased, punctuation marks will be removed as well as stopwords and finally words will be stemmed. 

Tokenization will then take place, a bag of words being created. Applying a sparsity threshold will only leave tokens that appear in at least 0.5 % of reviews. The bag of words takes the form of a Document Term Matrix, where the 1,000 rows correspond to the 1,000 reviews and there is a column for each token selected across the sparsity process. At the junction of each row and each column, there is a frequency number representing the number of occurrences of the corresponding token in the corresponding review. 

As a pre-attentive took, a wordcloud will show the most frequent tokens and that will be our first visual contact with data. 

```{r Adapting opts_chunk to enlarge display width for a wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Creating corpus and bag of words}
# Corpus is created on training reviews only to avoid any interference
# between training reviews and validation reviews. Otherwise, 
# tokens from validation set could slightly impact token selection
# when applying the sparsity threshold. 
reviews_train <- reviews[ind_train, ]
corpus <- VCorpus(VectorSource(reviews_train$text)) 

# Lowercasing, removing punctuation and stopwords, stemming document.
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# Managing sparsity with sparsity threshold. 
sparse <- removeSparseTerms(dtm, 0.995)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(sparse)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))

# Results from NLP process need checking. Let's build up 
# a wordcloud with the most frequent tokens.
set.seed(1)
wordcloud(colnames(sentSparse), colSums(sentSparse), min.freq = 10, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Readapting opts_chunk to go back to 60% width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

Some tokens were not expected, such as "dont" or "ive", since they seem to originate in short forms and were expected to have been eliminated as stopwords. 

<br>

### B. Checking up NLP Output

<br>

Let's start investigating with "dont". How many rows contain " dont "?

<br>

```{r Investigating "dont"}
# Which rows from training reviews have a 1 in the column "dont"? This 
# would mean that the corresponding NLP-transformed reviews contain " dont ".
bin <- which(sentSparse$dont == 1)

# How many rows contain " dont "?
df <- data.frame(length(bin)) %>% 
  `colnames<-`('OCCURRENCE OF "dont"') %>%
  `rownames<-`("Bag of Words")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2") 
rm(df)
```

<br>

Let's have a look at the first review that has produced "dont".

<br>

```{r First review producing "dont"}
df <- data.frame(reviews_train$text[bin[1]]) %>%
  `colnames<-`('FIRST REVIEW GENERATING "dont"') %>%
  `rownames<-`("Training Set")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2") 
rm(df)
```

<br>

"dont" contains a spelling error or is "alternative" grammar. Nevertheless, ideally, it should be treated as a short form from standard grammar, i.e. as the short form "don't". Consequently, if we want to eradicate short forms, we'll have to complement
stopwords with variants such as "dont", "couldnt". This will be done under the form of an additional stopword file called "stopwords_remaining.csv". 

Let's see the second review that, after NLP, has generated "dont".

<br>

```{r Second review generating "dont"}
df <- data.frame(reviews_train$text[bin[2]]) %>% 
  `colnames<-`('SECOND REVIEW GENERATING "dont"') %>%
  `rownames<-`("Training Set")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2") 
rm(bin, df)
```

<br>

This is another scenario: "don't" has been written in the standard way, but all punctuation marks have been removed, consequently it has become "dont" and is no longer identical to the stopword "don't" and will not be removed. Consequently, stopwords containing apostrophes should be removed before removing punctuation. Or punctuation marks should be removed except for apostrophes and hyphens by using parameters of the function removePunctuation() or other coding. A solution will be applied. 

Let's now analyze all tokens emanating from reviews_train, not just the most frequent ones. For brevity, only impactful results will be showcased. There are several unigrams that seem to originate from bigrams, e.g. "brokeni" at row 24.

<br>

```{r Row numbers of tokens including "brokeni"}
# Collecting all tokens.
tokens <- findFreqTerms(dtm, lowfreq = 1)

# Determining the number of columns of the presentation table. 
nc <- 5           

# Calculating the number of missing values to get a full matrix
# and adding hyphens accordingly to get a full matrix. 
mis <- ((ceiling(length(tokens) / nc)) * nc) - length(tokens)
tokens <- as.character(c(tokens, rep("-", mis)))
tokens <- data.frame(matrix(tokens, ncol = nc, byrow = TRUE)) %>%
  `colnames<-`(NULL) 
rm(nc, mis)

# Printing the row containing "brokeni".
knitr::kable(tokens[24, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2:6, bold = T, color = "#08457E", background = "#9bc4e2") 
```

<br>

Where does "brokeni" come from?

<br>

```{r Review generating "brokeni"}
v <- 1:nrow(reviews_train)
string <- "brokeni"
for(i in 1:nrow(reviews_train)) {
  v[i] <- length(grep(string, corpus[[i]]$content))
}

df <- data.frame(reviews_train$text[which(v == 1)], stringsAsFactors = FALSE) %>%
  `colnames<-`('REVIEW PRODUCING "brokeni"') %>%
  `rownames<-`("Training Set")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2") 
rm(v, string, i, df)
```

<br>

What happened? Well, "broken...I" was first lowercased to "broken...i", then punctuation was removed by the function removePunctuation, which does not insert any 
white space character, and "broken...i" has become "brokeni". 

In further text mining and machine learning steps, "brokeni" will be treated differently than "broken", which can be seen on the same row, i.e. on row 24 of the tokens from the training reviews.

This has to be changed: instead of the function removePunctuation(), specific and different for loops will be developed, replacing punctuation marks with white space characters instead of just removing punctuation marks and allowing for differentiated treatment of on the one hand apostrophes and on the other hand other punctuation marks.

<br>

Therefore, the preprocessing process will be rerun with for loops that replace punctuation marks with white space characters and allowing for differentiated treatment of on the one hand apostrophes and hyphens (dashes) and on the other hand other punctuation marks.

<br>

### C. Fine Tuning NLP

<br>

Instead of one stopword list generated by the function stopword("english"), we'll split it into four files while complementing a little bit the list. This aims at providing operational flexibility in further NLP and differentiated treatment of tokens in text mining. 

These are the four files:
- short_forms_pos.csv, with all positive short forms from stopwords("english") such as "she's", a few additional ones and numerous deliberately misspelled variants such as "she s" or "shes";
- short_forms_neg.csv, in the same approach, for short forms such as "isn't", "daren't" but also "isn t", "isnt", etc.;
- negation.csv, with seven negational unigrams such as "not" or "no";
- stopwords_remaining.csv, which is self-explanatory.

The 4 files have been uploaded to my GitHub repository, https://github.com/Dev-P-L/Sentiment-Analysis. They are going to be downloaded now and integrated into the NLP transformation.

Let's rebuild the corpus, the bag of words and the wordcloud. 

```{r Adapting opts_chunk again to enlarge display width for a wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Complementing NLP}
myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/short_forms_pos.csv"
short_forms_pos <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
short_forms_pos <- short_forms_pos[, 2] %>% as.vector()
# Normalizing (among others apostrophes). 
short_forms_pos <- sapply(short_forms_pos, utf8_normalize, map_quote = TRUE)

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/short_forms_neg.csv"
short_forms_neg <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
short_forms_neg <- short_forms_neg[, 2] %>% as.vector()
# Normalizing (among others apostrophes). 
short_forms_neg <- sapply(short_forms_neg, utf8_normalize, map_quote = TRUE)

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/negation.csv"
negation <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
negation <- negation[, 2] %>% as.vector()

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/stopwords_remaining.csv"
stopwords_remaining <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
stopwords_remaining <- stopwords_remaining[, 2] %>% as.vector()
rm(myfile)

# Creating and preprocessing corpus.
corpus_av0 <- VCorpus(VectorSource(reviews_train$text)) 
corpus_av0 <- tm_map(corpus_av0, content_transformer(tolower))

# Replacing all punctuation marks other than apostrophes with white space 
# characters, instead of simply suppressing punctuation marks, not to risk 
# collapsing two or more words into one. Keeping apostrophes 
# to leave intact short forms such as "don't" and be able to remove them.
for (i in 1:nrow(reviews_train)) {
  corpus_av0[[i]]$content <- gsub("(?!')[[:punct:]]", " ", 
                              corpus_av0[[i]]$content, perl = TRUE)
}
rm(i)

# Removing short forms after removing extra white space characters (all 
# white spaces except one of them in a sequence of white space characters). 
corpus_av0 <- tm_map(corpus_av0, stripWhitespace)
corpus_av0 <- tm_map(corpus_av0, removeWords, short_forms_neg)
corpus_av0 <- tm_map(corpus_av0, removeWords, short_forms_pos)

# Replacing all remaining apostrophes with white space characters (there might
# be other apostrophes than in short forms...). 
for (i in 1:nrow(reviews_train)) {
  corpus_av0[[i]]$content <- gsub("[[:punct:]]", " ", corpus_av0[[i]]$content)
}
rm(i)

# Removing n-grams from other files. 
corpus_av0 <- tm_map(corpus_av0, removeWords, negation)
corpus_av0 <- tm_map(corpus_av0, removeWords, stopwords_remaining)

# Stemming words.
corpus_av0 <- tm_map(corpus_av0, stemDocument)

# Removing numbers and extra white space characters.
corpus_av0 <- tm_map(corpus_av0, removeNumbers)
corpus_av0 <- tm_map(corpus_av0, stripWhitespace)

# Building up a bag of words in a Document Term Matrix.
dtm_av0 <- DocumentTermMatrix(corpus_av0)

# Managing sparsity with the sparsity threshold. 
sparse_av0 <- removeSparseTerms(dtm_av0, 0.995)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse_av0 <- as.data.frame(as.matrix(sparse_av0)) 

# Making all column names R-friendly.
colnames(sentSparse_av0) <- make.names(colnames(sentSparse_av0))

# Let's check whether shortcomings have disappeared or not. Let's build up 
# a wordcloud with the most frequent tokens from the training reviews.
set.seed(1)
wordcloud(colnames(sentSparse_av0), colSums(sentSparse_av0), min.freq = 10, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Readapting opts_chunk again to go back to 60% width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

In the wordcloud, there is no more token originating from short forms. Let's have a broader look at all tokens and build up a presentation table. "dont" has indeed disappeared. Let's check it up in the bag of words. 

<br>

```{r Checking whether "dont" has indeed disappeared}
# Retrieving all tokens.
tokens <- findFreqTerms(dtm_av0, lowfreq = 1)

# Choosing the number of columns of the presentation table. 
nc <- 5

# Calculating the number of missing tokens to have a full matrix. 
mis <- ((ceiling(length(tokens) / nc)) * nc) - length(tokens)

# Builing up table.
tokens <- as.character(c(tokens, (rep("-", mis))))
tokens <- data.frame(matrix(tokens, ncol = nc, byrow = TRUE)) %>%
  `colnames<-`(NULL) %>% `rownames<-`(NULL)

# Looking for "dont".
knitr::kable(tokens[50, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2:6, bold = T, color = "#08457E", background = "#9bc4e2")
rm(nc, mis)
```

<br>

So has "ive"!

<br>

```{r Showing that "ive" has disappeared}
knitr::kable(tokens[96, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2:6, bold = T, color = "#08457E", background = "#9bc4e2")
```

<br>

The same again for "brokeni".

```{r Showing that "brokeni" has disappeared}
knitr::kable(tokens[21, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2:6, bold = T, color = "#08457E", background = "#9bc4e2")
```

<br>

And "abovepretti" has also vanished.

<br>

```{r Showing that "abovepretti" has disappeared}
knitr::kable(tokens[1, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#08457E", background = "#9bc4e2") 
```

<br>

As well as many other oddities. I leave uncorrected some spelling errors, such as among others "disapoint" or "dissapoint" because this seems marginal. 

Let's have a first try at predicting sentiment on the basis of sentSparse_av0, which is our training set. 

<br>

### D. Measuring NLP Impact on Machine Learning Accuracy

<br>

The chosen machine learning model will be CART: it runs rather quickly and delivers clear decision trees. Running function rpart() on the training set delivers the accuracy level mentioned hereunder. 

<br>

```{r Running rpart() for the first time on the training set}
# Adding dependent variable.
sentSparse_av0 <- sentSparse_av0 %>% mutate(sentiment = reviews_train$sentiment)

# Training CART with the algorithm rpart.
set.seed(1)
fit_cart_av0 <- rpart(sentiment ~., data = sentSparse_av0)
fitted_cart_av0 <- predict(fit_cart_av0, type = "class")
cm_cart_av0 <- confusionMatrix(fitted_cart_av0, sentSparse_av0$sentiment)

# The accuracy level is 
df <- data.frame(round(cm_cart_av0$overall["Accuracy"], 4)) %>%
  `rownames<-`("Model: CART") %>% `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(df)
```

<br>

Now let's train the rpart method with the train() function from the caret package. 

By default, the train() function would train across 3 values of cp, i.e. the complexity parameter, resampling method, and with a resampling method based on 25 bootstrapped resamples for each tuned value of cp. 

The default resampling method seems appropriate in this context. The size of each resample will be the same of the size of the training set, i.e. 668. Not working with a smaller number of rows matters because 668 is already a limited size. Working with e.g. k-fold cross-validation would imply further splitting the training set. By the way, let's remember that resampling is sampling with replacement, some reviews being picked up twice or more and some other reviews not being selected. 

As far as the number of tuned values is concerned, let's upgrade it to 15 to increase the odds of improving accuracy? the more so since rpart runs rather quickly. 

Will accuracy improve?

<br>

```{r Running rpart with train() and 15 as tuneLength}
set.seed(1)
fit_cart_tuned_av0 <- train(sentiment ~ .,
                         method = "rpart",
                         data = sentSparse_av0,
                         tuneLength = 15,
                         metric = "Accuracy")
fitted_cart_tuned_av0 <- predict(fit_cart_tuned_av0)
cm_cart_tuned_av0 <- confusionMatrix(as.factor(fitted_cart_tuned_av0), 
                                     as.factor(sentSparse_av0$sentiment))

# The tuned rpart model delivers an accuracy level of
df <- data.frame(round(cm_cart_tuned_av0$overall["Accuracy"], 4)) %>%
  `rownames<-`("Model: CART + cp Tuning") %>% 
  `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(df)
```

<br>

Accuracy increases somewhat. Consequently, this method will be further used as a yardstick when further moving into NLP and text mining. 

For the record, let's have a look at a graph showing how accuracy evolves across the 15 cp values chosen by the train() function. 

<br>

```{r Graph about accuracy across cp values on resamples}
graph <-  
  ggplot(fit_cart_tuned_av0) + 
  geom_line(col = "blue", size = 1) +
  geom_point(col = "blue", size = 4) +
  ggtitle("Average Bootstrap Accuracy across cp Values") +
  xlab("Complexity Parameter") + ylab("Average Accuracy (Bootstrap)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

On the graph above, maximum accuracy is a bit different than the than the level previously indicated, actually a bit lower. Why is it different? Because on the graph it is, for each cp value, the average accuracy on the 25 bootstrapped resamples while accuracy previously given related to the whole training set.

The optimal value of cp is near zero. Is it really zero? 

<br>

```{r Optimal cp value}
# Rounding at 4 decimals and asking to display 4 decimals. 
df <- data.frame(sprintf("%.4f", round(fit_cart_tuned_av0$bestTune, 4))) %>%
      `colnames<-`("OPTIMAL cp VALUE") %>%
      `rownames<-`("Model: CART + cp Tuning")
knitr::kable(df, "html", align = "c") %>% 
      kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
      column_spec(1, bold = T, color = "#808080") %>%
      column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(df)
```

<br>

Yes, it is zero. This means that the train() function has kept the decision tree as complex as possible by assigning a zero value to the complexity parameter. 

On the whole training set, the rpart model delivers 78 % accuracy and the rpart model with tuning gives 79 %. Both levels are substantially higher than accuracy provided by the baseline model. The baseline model would predict a positive evaluation for all reviews (or alternatively a negative evaluation for all reviews). What accuracy level would the baseline model deliver? 

<br>

```{r Accuracy from baseline model}
df <- sentSparse_av0
pred_baseline <- 
  data.frame(sentiment = rep(" Pos", nrow(df))) %>%
  mutate(sentiment = factor(sentiment, levels = levels(df$sentiment)))
cm_baseline <- confusionMatrix(pred_baseline$sentiment, 
                               as.factor(df$sentiment)) 
df <- data.frame(sprintf("%.4f", round(cm_baseline$overall["Accuracy"], 4))) %>%
      `colnames<-`("ACCURACY ON THE TRAINING SET") %>%
      `rownames<-`("Model: Baseline")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(df)
```

<br>

Let's summarize results from the three models, not only with accuracy but also with additional performance metrics. 

<br>

```{r Summary table of the previous 3 models}
colname <- c("MODEL ID", "SHORT DESCRIPTION", "ACCURACY", "SENSITIVITY", 
             "NEG PRED VAL", "SPECIFICITY", "POS PRED VAL")
models <- c("baseline", "cart_av0", "cart_tuned_av0")
description <- c("baseline model", "CART", "CART + tuning")
cm <- c("cm_baseline", "cm_cart_av0", "cm_cart_tuned_av0")
tab <- data.frame(matrix(1:(length(colname) * length(models)),
                         ncol = length(colname), nrow = length(models)) * 1)
i <- 1
for (i in 1:length(models)) {
  tab[i, 1] <- models[i]
  tab[i, 2] <- description[i]
  tab[i, 3] <- 
    eval(parse(text = paste(cm[i], "$overall['Accuracy']", sep = "")))
  tab[i, 4] <- 
    eval(parse(text = paste(cm[i], "$byClass['Sensitivity']", sep = "")))
  tab[i, 5] <- 
    eval(parse(text = paste(cm[i], "$byClass['Neg Pred Value']", sep = "")))
  tab[i, 6] <- 
    eval(parse(text = paste(cm[i], "$byClass['Specificity']", sep = "")))
  tab[i, 7] <- 
    eval(parse(text = paste(cm[i], "$byClass['Pos Pred Value']", sep = "")))
}                 

# Neg Pred Val is indeterminate for the baseline model since it is 
# the result from a division by zero. Let's first assign a fake value 
# to easily round all columns. 
tab[1, 5] <- 0
tab_av0 <- tab %>% mutate_at(vars(3:7), funs(round(., 4))) %>%
           `colnames<-`(colname)
# Indicating true nature of result of Neg Pred Val for baseline model.
tab_av0[1, 5] <- "Div. by 0"  
knitr::kable(tab_av0, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, strikeout = T, 
           color = "#08457E", background = "#9bc4e2") %>%
  row_spec(2, bold = T, color = "#08457E", background = "#9bc4e2") %>%
  row_spec(3, bold = T, color = "#205030", background = "#a7e3bb")
rm(cm_baseline, pred_baseline, models, description, cm, tab)
```

<br>

In the table above, on row 1, fonts have been blurred into light blue and have been stricken through to indicate that this model has been discarded. The other two models should be seen as a cumulative process bringing accuracy improvement in a stepwise and incremental way, with the one on #50c878 background being the best in accuracy.

Accuracy with the baseline model is 50 %, which reflects prevalence. Models 2 and 3 are sensibly higher in accuracy. Model 2 gives 78 % and model 3 79 % in accuracy. 

With model 2 or 3, sensitivity and negative predictive value are lower than specificity and positive predictive value. This reflects false negatives being more numerous than false positives. False negatives are predictions pointing to "Neg" while the reference value is " Pos". This is an insight for text mining: perusing the false negatives and coming with some improvement. Imbalance is a little reduced with model 3 in comparison with model 2.

In order to confirm that false negatives are more numerous than false positives, let's have a look at the confusion matrix for both models. First, the confusion matrix from the rpart model without tuning. 

<br>

```{r Confusion matrix for the rpart model without tuning}
name <- c("TP = ", "FN = ", "FP = ", "TN = ")
tab <- table(fitted_cart_av0, sentSparse_av0$sentiment) %>% 
  as.vector() %>% paste(name, ., sep = "")
tab <- data.frame(matrix(tab, ncol = 2, nrow = 2, byrow = FALSE)) %>%
  `colnames<-`(c("Actually positive", "Actually negative")) %>%
  `rownames<-`(c("Predicted positive with CART", 
                 "Predicted negative with CART"))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "black") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  column_spec(3, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

The weak point lies in the first column, on #c90016 background: the relatively high number of false negatives and, as a corollary, the relatively low number of true positives. On the reference positive class (" Pos" in label), predicting  seems problematic or at the very least challenging: false negatives are rife. On the contrary, on the reference negative class ("Neg" in label), predicting  has run smoothly, with a satisfactorily low number of false positives. 

The tuned rpart model is expected to to slightly reduced the excess in false negatives.

<br>

```{r Confusion matrix for the rpart model with tuning}
name <- c("TP = ", "FN = ", "FP = ", "TN = ")
tab <- table(fitted_cart_tuned_av0, sentSparse_av0$sentiment) %>% 
  as.vector() %>% paste(name, ., sep = "")
tab <- data.frame(matrix(tab, ncol = 2, nrow = 2, byrow = FALSE)) %>%
  `colnames<-`(c("Actually positive", "Actually negative")) %>%
  `rownames<-`(c("Predicted positive with CART + tuning", 
                 "Predicted negative with CART + tuning"))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "black") %>%
  column_spec(2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  column_spec(3, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab, tokens, name)
rm(cm_cart_av0, fit_cart_av0, fitted_cart_av0)
```

<br>

With the tuned rpart model, globally accuracy has slightly improved: the sum of numbers on the main diagonal is larger. 

On the #c90016 background figure, predicting on the reference positive class is less prolific in false negatives and, as a corollary, true positives are more predominant. 

On the secondary diagonal, imbalance between false negatives and false positives is less marked, not only because false negatives are less numerous but also because false positives are a bit more numerous. Nevertheless false negatives remain the weak point, being twice as numerous as false positives. 

False negatives - and false positives - will be perused through text mining in the next section, looking for new insights towards accuracy improvement. 

<br>

## III. TEXT MINING TOWARDS HIGHER ACCURACY

<br>

Let's now have a look at the false negatives from the CART model with tuning, which constitute a challenge.

<br>

### A. Visualizing False Negatives

<br>

```{r False negatives from model cart_tuned}
df <- sentSparse_av0 %>% mutate(pred = fitted_cart_tuned_av0) %>% as.data.frame()
FN_train <- ifelse(df$sentiment == " Pos", 1, 0) - 
            ifelse(df$pred == " Pos", 1, 0)
FN_train <- ifelse(FN_train == 1, 1, 0)

# Let's build up a random sample taken from false negatives. 
sample_size <- 12
set.seed(1)
seq <- sort(sample(which(FN_train == 1), sample_size, replace = FALSE))
rm(FN_train)

# Let's build up a presentation table.
df <- data.frame(matrix(nrow = length(seq), ncol = 2) * 1)

for (i in 1:length(seq)) {
  row <- as.numeric(seq[i])
  df[i, 1] <- reviews_train[row, 1]
  df[i, 2] <- corpus_av0[[row]]$content
}
rm(sample_size, i, row)

colname_token <- c("Review Corresponding to a FALSE NEGATIVE with CART + Tuning", "After NLP", "Usable Subjective Information")
comment <- c("super", "fast + faster", "prettier + sharp", "infatu", "wise",
             "awesom", "rock", "[no trouble]", "[knows what they're doing]", 
             "[as described]", "secur", "quick")
df <- df %>% mutate(com = comment, ro = 1:nrow(.)) %>% 
             select(ro, everything()) %>% `colnames<-`(c("", colname_token)) 
rm(seq, comment)

# Let's save df under a less anonymous name for further use and print it. 
df_FN_cart <- df %>% as.data.frame() 
rm(df)
tab <- kable(df_FN_cart, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(c(1:7, 11:12), bold = T, color = "#08457E", background = "#9bc4e2") %>%
  row_spec(8, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(9:10, bold = T, color = "#333333", background = "#b2b2b2") 
tab
rm(tab)
```

<br>

There are three scenarios, each illustrated with a different background color. Let's get started with the first scenario on the blue background. 

<br>

### B. Subjective Information Unigrams

<br>

On the blue background in the table above, there 9 cases out of 12. In each review, there is at least one token with subjective information that is clear from a human point of view. Let's just pinpoint "super", "fast", "faster", "prettier", "infatuated", "wise", "awesome", "rocks", etc., or, after stemming "super", "fast", "faster", "prettier", "infatu", "wise", "awesom", "rock", etc. 

These words, and the related standardized tokens, can be classified in two categories: - compliance-related tokens, expressing compliance or incompliance with 
expectations, requirements or advertisements ("fast", "faster", "rocks", etc.),
- sentiment-related tokens other than in previous category ("infatuated").

The difference between the two categories can sometimes be tricky and both categories
will be referred to altogether in this project using phrases such as "subjective information" or "tokens conveying subjective information". 

"Unfortunately", the tokens pinpointed among the false negatives do not show in the decision trees. Let's visualize the decision tree from the CART model with tuning. 

<br>

```{r Broadening display to better print a decision tree}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Decision tree from CART with tuning}
prp(fit_cart_tuned_av0$finalModel, uniform = TRUE, cex = 0.6, 
    box.palette = "auto")
```

```{r Readapting opts_chunk after decision tree to have 60% width again for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br>

What types of tokens can be seen in the decision tree? There is a majority of tokens conveying subjective information ("great", "comfort", "love", "like", disappoi", etc.). They are usually the highest ranked. There are other types of tokens:
- intent-related tokens ("purchas", "buy") or
- topic-related tokens ("plug", "ear").

Which is an interesting insight. In CART, tokens conveying subjective information predominate, which is not at all surprising! This points to solutions allocating higher priority to tokens conveying subjective information. 

Although the majority of tokens are conveying subjective information in the decision tree, the same type tokens present among the false negatives do not show. It is probably a matter of word (or token) frequency. This can be first checked up in the wordcloud that has already been visualized. 

```{r Adapting opts_chunk to print a wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Wordcloud}
df <- sentSparse_av0[, - ncol(sentSparse_av0)]
set.seed(1)
wordcloud(colnames(df), colSums(df), 
          min.freq = 10, max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
rm(df)
```

```{r Readapting opts_chunk to have 60% width again for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

The wordcloud, which requires a minimal frequency of 10, is not comprised of the tokens pinpointed among the false negatives. 

Among tokens depicted in the wordcloud, there are 
- topic-related tokens ("phone", "batteri", "headset", "sound", "ear", etc.),
- intent-related tokens ("purchas", "buy", "bought"),
- compliance-related tokens, expressing compliance or incompliance with 
expectations, requirements or advertisements ("fit", "comfort", "problem", etc.),
- sentiment-related tokens other than in previous category ("love", "like", etc.).

"phone", which is topic-related, is the token with the 
highest frequency. Other topic-related tokens rank among the highest 
frequencies: "batteri", "product", "headset", "sound", etc. 
"phone" has the highest frequency in the wordcloud but does not show in the decision tree. The same holds for other highly ranked topic-related tokens. 

This looks like an interesting insight. Token frequency should not be the only criterion. Actually, there should be some hierarchy between types of tokens (subjective information, intent-related, topic-related). This does not necessarily mean a strict hierarchy starting with types and then going down to tokens; there could be some intermixture, token frequency could coexist with prioritization of subjective information. 

For illustrative purposes, tokens can be visualized in decreasing order of frequency
in the graph below.

<br>

```{r Token frequency histogram}
df <- sentSparse_av0[, - ncol(sentSparse_av0)]
freq <- data.frame(to = colnames(df), fre = as.integer(colSums(df)), 
                   stringsAsFactors = FALSE) %>% 
        arrange(desc(fre)) %>% head(., 30)
graph <-  freq %>% mutate(to = reorder(to, fre)) %>%
  ggplot(aes(to, fre)) + 
  geom_bar(stat = "identity", width = 0.80, color = "#007ba7", fill = "#007ba7") + 
  coord_flip() +
  ggtitle("Token Frequency") +
  xlab("Token") + ylab("Frequency") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(df, freq, graph)
```

<br>

In conclusion, it might be impactful to garner subjective information conveyed by tokens such as "super". Since CART doesn't do it, why not replace such tokens with either a generic positive sentiment token or a generic negative token? Examples of positive minded tokens can be "super", "faster", "prettier", "infatu", "awesom", etc.). This would reduce the number of tokens conveying subjective information and provide rather high frequencies for both generic tokens. 

In this project, polarity of some tokens conveying subjective information will be inserted in additional files. That is one avenue of research.

<br>

### C. Negation Handling

<br>

Let's go back to the table above that is comprised of 12 reviews corresponding to false negatives in the model cart_tuned. More particularly, let's retrieve the review with a negation form. 

<br>

```{r False negatives from model cart_tuned_av0 model}
tab <- df_FN_cart[8, ]
tab <- kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#205030", background = "#a7e3bb") 
tab
rm(tab)
```

<br>

The bigram "no trouble" is clear from a human point of view but this bigram has become "troubl", the negational token "no" having been removed with all other stopwords. Even if "troubl" is polarized under a generic negative token, as suggested above, the right polarity of "no trouble" wouldn't show. Two avenues are opened up: the whole bigram "no trouble" could be converted into a generic positive token or, more generally, negational stopwords such as "not" or "no" should no longer be removed, which is another avenue for improvement. 

In this sample, there is only one case out of 12 with an ignored negational phrase. Is it worthwhile heading for properly dealing with negational unigrams? Let's integrate negational unigrams into the bag of words and produce a wordcloud in order to get some pre-attentive insight about frequency of negational unigrams such as "not" or "no".  

```{r Bag of words integrating negational unigrams}
corpus_2 <- VCorpus(VectorSource(reviews_train$text)) 
corpus_2 <- tm_map(corpus_2, content_transformer(tolower))

# Replacing all punctuation marks with white space characters 
# instead of simply removing punctuation marks 
# to prevent the process from generating tokens like "brokeni".
# Keeping apostrophes to leave intact short forms such as "don't",
# which can later match stopwords to be removed. 
for (i in 1:nrow(reviews_train)) {
  corpus_2[[i]]$content <- gsub("(?!')[[:punct:]]", " ", 
                                  corpus_2[[i]]$content, perl = TRUE)
}

corpus_2 <- tm_map(corpus_2, stripWhitespace)
corpus_2 <- tm_map(corpus_2, removeWords, short_forms_neg)
corpus_2 <- tm_map(corpus_2, removeWords, short_forms_pos)

# Removing apostrophes as well (there can be apostrophes outside of
# short forms). 
for (i in 1:nrow(reviews_train)) {
  corpus_2[[i]]$content <- gsub("[[:punct:]]", " ", corpus_2[[i]]$content)
}
rm(i)

# Further NLP without removing negational unigrams from 
# the file negation.csv, contrary to what had been done previously. 
corpus_2 <- tm_map(corpus_2, removeWords, stopwords_remaining)
corpus_2 <- tm_map(corpus_2, stemDocument)
corpus_2 <- tm_map(corpus_2, removeNumbers)
corpus_2 <- tm_map(corpus_2, stripWhitespace)

# Building up Document Term Matrix, applying sparsity threshold, 
# converting into data frame and getting R-friendy colnames. 
dtm_2 <- DocumentTermMatrix(corpus_2)
sparse_2 <- removeSparseTerms(dtm_2, 0.995)
sentSparse_2 <- as.data.frame(as.matrix(sparse_2)) 
colnames(sentSparse_2) <- make.names(colnames(sentSparse_2))
```

```{r Expanding display width for wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Wordcloud from bag of words integrating negational unigrams}
wordcloud(colnames(sentSparse_2), colSums(sentSparse_2), min.freq = 10, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Reducing display width back to 60 % for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

Would one negational unigrams appear in a wordcloud? "not" is indeed highly ranked, just below "phone". This is an insight advocating inclusion of negational unigrams. 

Let's have a look at word associations with "not": does it often change polarity of review? 

<br>

```{r Word associations with "not"}
df <- findAssocs(sparse_2, "not", 0.05)
df <- as.data.frame(df$not) %>% mutate(token = names(df$not)) %>%
  `colnames<-`(c("correlation", "token")) %>% select(token, correlation) %>%
  `colnames<-`(c("Token", 'Correlation with "not"'))
df <- kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  row_spec(1:nrow(df), bold = T, color = "#08457E", background = "#9bc4e2") 
df
rm(df)
```

<br>

The first stemmed unigram on the list is "impress". Let's localize the reviews producing "impress".

<br>

```{r Training reviews producing an association of "not" and "impress"}
# Localizing reviews containing "impress". 
v <- 1:nrow(reviews_train)
string <- "impress"
for(i in 1:nrow(reviews_train)) {
  v[i] <- length(grep(string, corpus_2[[i]]$content))
}
v <- which(v == 1)

# Let's prepare a table. 
df <- data.frame(matrix(1:(length(v) * 1), ncol = 1))
for(i in 1:length(v)) {
  df[i, 1] <- reviews_train[v[i], 1]
}

# Let's print the table.
colname <- c("", 'Training Review Containing "impressed"')
df <- df %>% mutate(ro = 1:nrow(.)) %>% select(ro, everything()) %>%
             `colnames<-`(colname)
kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(c(1, 3:4), bold = T, color = "#08457E", background = "#9bc4e2") %>%
  row_spec(c(2, 5:6), bold = T, color = "#205030", background = "#a7e3bb") 

rm(string, df, v, i)
rm(corpus_2, dtm_2, sparse_2, sentSparse_2)
```

<br>

In the three training reviews on green background, negation unigrams completely change polarity of review. Among the training reviews, there are three reviews containing "not impressed". Moreover, in the second of these reviews, there is also "not recommend" and "not laughing". This illustrates the usefulness of including negational words in one way or another. 

Reference has been made to negational unigrams because negation can also be encapsulated into short forms (also called "contractions"). This sample of false negatives, which has been selected at random, is not comprised of negative short forms. But the same procedure, applied to false positives, delivers several of them. 

<br>

```{r False positives from cart_tuned_av0 model}
# Let's localize the false positives originating from the tuned CART model.
df <- sentSparse_av0 %>% mutate(pred = fitted_cart_tuned_av0) %>% as.data.frame()
FP_train <- ifelse(df$sentiment == " Pos", 1, 0) - 
            ifelse(df$pred == " Pos", 1, 0)
FP_train <- ifelse(FP_train == -1, 1, 0)

# Let's generate a sample index of false positives at random. 
sample_size <- 12
set.seed(1)
seq <- sort(sample(which(FP_train == 1), sample_size, replace = FALSE))
rm(FP_train)

# Let's organize a presentation table, retrieve reviews corresponding 
# to the sample index, prepare and print a presentation table.
df <- data.frame(matrix(nrow = sample_size, ncol = 2) * 1)

for (i in 1:length(seq)) {
  row <- as.numeric(seq[i])
  df[i, 1] <- reviews_train[row, 1]
  df[i, 2] <- corpus_av0[[row]]$content
}

colname_token <- c("Review Corresponding to a FALSE POSITIVE with CART + Tuning", "After NLP", "Usable Subjective Information")
comment <- c("not a good", "whine + ? the less", "shouldn't", 
             "slow + crawl + lock-up", "did not work well", "still waiting", 
             "terrible", "difficult", "wasn't always easy", "sorry", 
             "not as good", "don't like")
df <- df %>% mutate(com = comment, ro = 1:nrow(.)) %>% 
             select(ro, everything()) %>% 
             `colnames<-`(c("", colname_token))
df_FP_cart <- df
kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(c(8, 10), bold = T, color = "#08457E", background = "#9bc4e2") %>%
  row_spec(c(1, 5, 9, 11, 12), bold = T, color = "#205030", 
           background = "#a7e3bb") %>%
  row_spec(c(2, 3, 4, 6:7), bold = T, color = "#333333", 
           background = "#d8d8d8") 

rm(df, i, row, sample_size, seq)
```

<br>

In six cases, there is negation, which the machine learning could not take into account since these negation forms had been removed through NLP. There are four negative short forms. This shows the importance of negational forms, be it under the form of negational unigrams ("no", etc.) or of negational short forms.

For the record, blue background has been used at least one token per review expresses sentiment orientation, from a human point of view: "difficult" or "sorry". Recommendation: replacing with one generic token expressing criticism. #ede1f5 background has been used in simple negation scenarios: there is negation and at least one unigram conveying positive subjective information. #c90016 background has been used for more complex cases, with e.g. sarcasm: these cases will be analyzed in the next section.

<br>

### D. Multigrams Conveying Subjective Information

Sentiment can also be expressed through associations of words, with difficulty gradience among cases.

In some cases, there are rather stereotyped phrases such as "knows what they're doing" (competency statement) or "as described" (compliance statement). Some of these phrases will be listed among positive multigrams and replaced with a generic positive token. Both examples have been found among false negatives.  

<br>

```{r False negatives from cart_tuned_av0 model with more difficult cases}
tab <- df_FN_cart[9:10, ]
tab <- kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:nrow(tab), bold = T, color = "#333333", background = "#d8d8d8") 
tab
rm(tab)
```

<br>

At the other end of the difficulty continuum, there are several training reviews ranked among false positives: reviews with figurative wording, sarcasm, irony, metaphors, multifaceted reviews, etc. 

<br>

```{r False positives from cart_tuned_av0 model with the most difficult cases}
tab <- df_FP_cart[c(2:4, 7), ]
tab <- kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:nrow(tab), bold = T, color = "#333333", background = "#d8d8d8") 
tab
rm(tab)
```

<br>

Examples: figurative "whine" instead of being disappointed, frustrated; figurative "crawl"; sarcasm and metaphors about monkeys; multifaceted review such as "My experience was terrible..... This was my fourth bluetooth headset, and while it was much more comfortable than my last Jabra (which I HATED!!!". 

What will be done here: replacing some unigrams or n-grams with one generic negative token: e.g. "whine", "shouldn't", "lock up", etc. 

<br>

### E. Topic-related Tokens

<br>

It has been seen that some topic-related tokens have high frequency but do not even appear in the decision tree. So, "phone" has the highest frequency but is not present in the decision tree. "batteri", "headset" do not even appear in the decision tree!

This difference between wordcloud and decision tree might be insightful. Should topic-related tokens be discarded except the few ones that appear in the decision tree? Should they be excluded to make the bag of words less messy? But would they systematically be excluded in e.g. other machine learning models? 

To get some confirmation or infirmation, let's apply another algorithm. A Random Forests model is chosen because it allows for some rough ranking of predictor impact. 
First, here is the accuracy level on the training set. 

<br>

```{r Runing Random Forest}
# Random Forest model
fit_rf_av0 <- train(sentiment ~ ., method = "rf", data = sentSparse_av0) 
fitted_rf_av0 <- predict(fit_rf_av0)

# Confusion matrix and other statistics
cm_rf_av0 <- confusionMatrix(as.factor(fitted_rf_av0), 
                             as.factor(sentSparse_av0$sentiment))

# Printing accuracy. 
tab <- data.frame(round(cm_rf_av0$overall["Accuracy"], 4)) %>%
  `rownames<-`("Model: Random Forest") %>% 
  `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)

# Getting predictor importance and saving it for further use.
df <- varImp(fit_rf_av0)
importance_rf <- data.frame(t = rownames(df$importance), 
                            i = round(df$importance$Overall, 2), 
                            stringsAsFactors = FALSE) %>% 
                 arrange(desc(i)) %>% 
                 `colnames<-`(c("Stemmed Token", 
                   "Predictor Importance in Random Forest")) %>% head(., 20)
rm(df)
```

<br>

Would the high accuracy level mean that this model solves all issues and that further text mining and further machine learning are both useless? I do not think so: very often, at least on the basis of my experience, rf has a tendency to overfitting, as well as CART either. Results on the validation set might be substantially lower. Consequently, text mining remains meaningful and worth performing. 

Here is predictor importance from Random Forest. 

<br>

```{r Printing predictor importance from Random Forest}
importance_rf <- knitr::kable(importance_rf, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"),  
                full_width = F, font_size = 16) %>%
  column_spec(1:2, bold = T, color = "#08457E", background = "#9bc4e2")
importance_rf
rm(importance_rf)
```

<br>

Actually, the ranking of predictors by impact is rather different than the ranking in the CART decision tree: e.g. "phone" is ranked in the 11th position. In the 16 first positions, there are three topic-related tokens ("price", "phone" and "product") while there is none in the 16 first positions in the decision tree. In a snapshot, three topic-related tokens get some importance in  Random Forest and Random Forest performs better: that is a reason to keep topic-related tokens in the bag of words. The more so since other models than CART will be trained in the section about machine learning. 

Consequently, topic-related tokens that do not show in the CART decision tree will not be discarded. 

Accuracy from Random Forest has outperformed CART accuracy; there are false negatives and positives with Random Forest though. Here is the confusion matrix from Random Forest. 

<br>

```{r Confusion matrix from Random Forest}
tab <- table(fitted_rf_av0, sentSparse_av0$sentiment) %>% as.vector()
tab <- data.frame(matrix(tab, ncol = 2, nrow = 2, byrow = FALSE)) %>%
  `colnames<-`(c("Actually positive", "Actually negative")) %>%
  `rownames<-`(c("Predicted positive with Random Forest", 
                 "Predicted negative with random Forest"))
tab <- knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2:3, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

As expected, the number of false negatives and false positives is more limited than with CART since accuracy is larger. It is also true for each group separately: less false negatives in Random Forest and less false positives. 

There still remain false negatives and false positives with the rf model. Let's have a look at the false negatives, which are more numerous.Here is a sample of the false negatives from Random Forest. 

<br>

```{r False negatives from Random Forest}
# First localizing false negatives originating from the rf model. 
df <- sentSparse_av0 %>% mutate(pred = fitted_rf_av0) %>% as.data.frame()
FN_train <- ifelse(df$sentiment == " Pos", 1, 0) - 
            ifelse(df$pred == " Pos", 1, 0)
FN_train <- ifelse(FN_train == 1, 1, 0)

# Second, creatig index at random to select some false negatives
# although the number of false negatives is rather limited,
# but it is a way to stick to a previously used procedure. 
sample_size <- 12
set.seed(1)
seq <- sort(sample(which(FN_train == 1), sample_size, replace = FALSE))

# Retrieving reviews and NLP-transformed reviews. 
df <- data.frame(matrix(nrow = sample_size, ncol = 2) * 1)
for (i in 1:length(seq)) {
  row <- as.numeric(seq[i])
  df[i, 1] <- reviews_train[row, 1]
  df[i, 2] <- corpus_av0[[row]]$content
}
rm(FN_train, seq, sample_size, i, row)

# Naming columns and commenting.
colname_token <- c("Review Corresponding to a FALSE NEGATIVE with Random Forest",                       "After NLP", "Usable Subjective Information")
comment <- c("simpler", "job done", "incred", "shouldv invent sooner", 
             "rock", "incredi", "fix problem", "fabul", "perfect", 
             "thumbs up", "any problem", "wonder")

# Finalizing and printing.
df <- df %>% mutate(com = comment) %>% `colnames<-`(colname_token)
kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(c(1, 3, 5:6, 8:9, 12), bold = T, color = "#08457E", 
           background = "#9bc4e2") %>%
  row_spec(11, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(c(2, 4, 7, 10), bold = T, color = "#333333", background = "#d8d8d8")

rm(colname_token, comment, df, df_FN_cart, df_FP_cart)
rm(cm_cart_tuned_av0, fit_cart_tuned_av0, fitted_cart_tuned_av0)
rm(cm_rf_av0, fit_rf_av0, fitted_rf_av0)
rm(corpus_av0, dtm_av0, sparse_av0)
```

<br>

In the current section, insights have been obtained
- from comparing token frequency between on the one hand wordcloud and histogram and on the other hand decision tree
- and from perusing false negatives and positives and pinpointing available, and hopefully usable, subjective information.

In the next section, these text mining insights will be tentatively transposed into NLP and machine learning actions towards more accuracy.

<br>

### F. Applying Insights from Text Mining

<br>

Three avenues of improvement have been opened up:

- integrating negational unigrams ("not", etc.);
- integrating negative short forms;
- polarizing n-grams.

Stepwise, the three avenues will be quantitatively tested. 

<br>

## IV. NLP RERUN WITH INSIGHTS FROM TEXT MINING

<br>

### A. Integrating Negational Unigrams

<br>

Negational unigrams will be introduced, NLP will be rerun as well as the CART model with tuning, which is used as a performance yardstick. Here are the results. 

<br>

```{r Negational unigrams such as not get in}
# Building up new corpus.
corpus_av1_a <- VCorpus(VectorSource(reviews_train$text)) 
corpus_av1_a <- tm_map(corpus_av1_a, content_transformer(tolower))

# Replacing all punctuation marks with white space characters,
# instead of just removing punctuation marks, 
# to prevent tokens like "brokeni" from being generated.
# Keeping apostrophes to leave intact short forms such as "don't"
# so that they can be removed as stopwords.  
for (i in 1:nrow(reviews_train)) {
  corpus_av1_a[[i]]$content <- gsub("(?!')[[:punct:]]", " ", 
                                    corpus_av1_a[[i]]$content, perl = TRUE)
}
rm(i)

# Removing short forms after regulating white space characters.
corpus_av1_a <- tm_map(corpus_av1_a, stripWhitespace)
corpus_av1_a <- tm_map(corpus_av1_a, removeWords, short_forms_neg)
corpus_av1_a <- tm_map(corpus_av1_a, removeWords, short_forms_pos)

# Removing remaining apostrophes (there can be apostrophes 
# outside of short forms). 
for (i in 1:nrow(reviews_train)) {
  corpus_av1_a[[i]]$content <- gsub("[[:punct:]]", " ", 
                                    corpus_av1_a[[i]]$content)
}
rm(i)

# Removing stopwords_remaining, stemming, 
# removing numbers, digits and multiple white space characters (leaving only
# one white space character at a time).
corpus_av1_a <- tm_map(corpus_av1_a, removeWords, stopwords_remaining)
corpus_av1_a <- tm_map(corpus_av1_a, stemDocument)
corpus_av1_a <- tm_map(corpus_av1_a, removeNumbers)
corpus_av1_a <- tm_map(corpus_av1_a, stripWhitespace)

# Building bag of words, managing sparsity threshold, converting
# to data frame, regularizing column names and adding dependent variable.
dtm_av1_a <- DocumentTermMatrix(corpus_av1_a)
sparse_av1_a <- removeSparseTerms(dtm_av1_a, 0.995)
sentSparse_av1_a <- as.data.frame(as.matrix(sparse_av1_a)) 
colnames(sentSparse_av1_a) <- make.names(colnames(sentSparse_av1_a))
sentSparse_av1_a <- sentSparse_av1_a %>% 
  mutate(sentiment = reviews_train$sentiment)

# Training CART with the algorithm rpart with cp tuning.
set.seed(1)
fit_cart_tuned_av1_a <- train(sentiment ~ .,
                              method = "rpart",
                              data = sentSparse_av1_a,
                              tuneLength = 15,
                              metric = "Accuracy")
fitted_cart_tuned_av1_a <- predict(fit_cart_tuned_av1_a)
cm_cart_tuned_av1_a <- confusionMatrix(as.factor(fitted_cart_tuned_av1_a), 
                      as.factor(sentSparse_av1_a$sentiment))

# Printing accuracy. 
tab <- data.frame(cm_cart_tuned_av1_a$overall["Accuracy"]) %>%
       `rownames<-`("Model: Neg Short Forms + CART + Tuning") %>%
       `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(tab, "html", align = "c") %>% 
       kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
       column_spec(1, bold = T, color = "#333333") %>%
       column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

There is accuracy improvement approximately from 79 % to 81 %. Consequently, negational unigrams such as "not" will be kept in the corpus. 

Does "not" show in the decision tree?

<br>

```{r Expanding display width in anticipation of decision tree}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Decision tree from tuned CART after adding negation such as not}
prp(fit_cart_tuned_av1_a$finalModel, uniform = TRUE, cex = 0.6, 
    box.palette = "auto")
```

```{r Reducing display width back to 60 %}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br>

### B. Adding Negative Short Forms

<br>

Now, negative short forms will be tested too. First, negative short forms will be introduced into the bag of words. 

<br>

```{r Negative short forms get in}
# Building up new corpus.
corpus_av1_b <- VCorpus(VectorSource(reviews_train$text)) 
corpus_av1_b <- tm_map(corpus_av1_b, content_transformer(tolower))

# Replacing all punctuation marks with white space characters,
# instead of just removing punctuation marks, 
# to prevent tokens like "brokeni" from being generated.
# Keeping apostrophes to leave intact positive short forms 
# such as "it's" so that they can be removed. 
for (i in 1:nrow(reviews_train)) {
  corpus_av1_b[[i]]$content <- gsub("(?!')[[:punct:]]", " ", 
                                    corpus_av1_b[[i]]$content, perl = TRUE)
}
rm(i)

# Removing only positive short forms after reducing to one the number of 
# white space characters in a row in a row.
corpus_av1_b <- tm_map(corpus_av1_b, stripWhitespace)
corpus_av1_b <- tm_map(corpus_av1_b, removeWords, short_forms_pos)

# Removing remaining apostrophes. 
for (i in 1:nrow(reviews_train)) {
  corpus_av1_b[[i]]$content <- gsub("[[:punct:]]", " ", 
                                    corpus_av1_b[[i]]$content)
}
rm(i)

# Removing stopwords_remaining, stemming, 
# removing numbers, digits and multiple white space characters (leaving only
# one white space character at a time).
corpus_av1_b <- tm_map(corpus_av1_b, removeWords, stopwords_remaining)
corpus_av1_b <- tm_map(corpus_av1_b, stemDocument)
corpus_av1_b <- tm_map(corpus_av1_b, removeNumbers)
corpus_av1_b <- tm_map(corpus_av1_b, stripWhitespace)

# Building bag of words, managing sparsity threshold, converting 
# to data frame, regularizing column names and adding dependent variable.
dtm_av1_b <- DocumentTermMatrix(corpus_av1_b)
sparse_av1_b <- removeSparseTerms(dtm_av1_b, 0.995)
sentSparse_av1_b <- as.data.frame(as.matrix(sparse_av1_b)) 
colnames(sentSparse_av1_b) <- make.names(colnames(sentSparse_av1_b))
sentSparse_av1_b <- sentSparse_av1_b %>% 
  mutate(sentiment = reviews_train$sentiment)

# Training CART with the algorithm rpart with cp tuning.
set.seed(1)
fit_cart_tuned_av1_b <- train(sentiment ~ .,
                              method = "rpart",
                              data = sentSparse_av1_b,
                              tuneLength = 15,
                              metric = "Accuracy")
fitted_cart_tuned_av1_b <- predict(fit_cart_tuned_av1_b)
cm_cart_tuned_av1_b <- confusionMatrix(as.factor(fitted_cart_tuned_av1_b), 
                                       as.factor(sentSparse_av1_b$sentiment))
# Printing accuracy.
tab <- data.frame(cm_cart_tuned_av1_b$overall["Accuracy"]) %>%
       `rownames<-`(
          "Model: Negation + Neg Short Forms + CART + Tuning") %>%
       `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

Adding negative short forms such as "isn't" to negation such as "not" does not impact accuracy level. Now, instead of being just added, negative short forms will be replaced with "not". 

<br>

```{r Replacing negative short forms with "not"}
# Building up new corpus.
corpus_av1_c <- VCorpus(VectorSource(reviews_train$text)) 
corpus_av1_c <- tm_map(corpus_av1_c, content_transformer(tolower))

# Replacing all punctuation marks with white space characters,
# instead of just removing punctuation marks, 
# to prevent tokens like "brokeni" from being generated.
# Keeping apostrophes to leave intact short forms such as "it's"
# so that positive short forms can be removed. 
for (i in 1:nrow(reviews_train)) {
  corpus_av1_c[[i]]$content <- gsub("(?!')[[:punct:]]", " ", 
                                    corpus_av1_c[[i]]$content, perl = TRUE)
}
rm(i)

# Replacing negative short forms with "not".
for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(short_forms_neg)) {
    corpus_av1_c[[i]]$content <- gsub(short_forms_neg[j], " not ", 
                                   corpus_av1_c[[i]]$content)
  }
}

# Removing only positive short forms after reducing to one the number of 
# white space characters in a row in a row.
corpus_av1_c <- tm_map(corpus_av1_c, stripWhitespace)
corpus_av1_c <- tm_map(corpus_av1_c, removeWords, short_forms_pos)

# Removing remaining apostrophes. 
for (i in 1:nrow(reviews_train)) {
  corpus_av1_c[[i]]$content <- gsub("[[:punct:]]", " ", 
                                    corpus_av1_c[[i]]$content)
}
rm(i)

# Removing stopwords_remaining, stemming, removing numbers, 
# digits and multiple white space characters (leaving only
# one white space character at a time).
corpus_av1_c <- tm_map(corpus_av1_c, removeWords, stopwords_remaining)
corpus_av1_c <- tm_map(corpus_av1_c, stemDocument)
corpus_av1_c <- tm_map(corpus_av1_c, removeNumbers)
corpus_av1_c <- tm_map(corpus_av1_c, stripWhitespace)

# Building bag of words, managing sparsity threshold, converting 
# to data frame, regularizing column names and adding dependent variable.
dtm_av1_c <- DocumentTermMatrix(corpus_av1_c)
sparse_av1_c <- removeSparseTerms(dtm_av1_c, 0.995)
sentSparse_av1_c <- as.data.frame(as.matrix(sparse_av1_c)) 
colnames(sentSparse_av1_c) <- make.names(colnames(sentSparse_av1_c))
sentSparse_av1_c <- sentSparse_av1_c %>% 
  mutate(sentiment = reviews_train$sentiment)

# Training CART with the algorithm rpart with cp tuning.
set.seed(1)
fit_cart_tuned_av1_c <- train(sentiment ~ .,
                              method = "rpart",
                              data = sentSparse_av1_c,
                              tuneLength = 15,
                              metric = "Accuracy")
fitted_cart_tuned_av1_c <- predict(fit_cart_tuned_av1_c)
cm_cart_tuned_av1_c <- confusionMatrix(as.factor(fitted_cart_tuned_av1_c), 
                                       as.factor(sentSparse_av1_c$sentiment))
# Printing accuracy.
tab <- data.frame(cm_cart_tuned_av1_c$overall["Accuracy"]) %>%
  `rownames<-`('Model: Negation + Neg Short Forms = "not" + CART + Tuning') %>%
  `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

Replacing negative short forms with "not" downgrades accuracy. This path will not be followed; problematic will be pursued in next section. 

<br>

### C. Polarization - Text Classsification - Text Substitution

<br>

In samples of false negatives and false positives, analysis has pinpointed unigrams and multigrams that convey some subjective information. They have been listed and classified in positive and negative. They have been stopped in four files and uploaded in the GitHub repository https://github.com/Dev-P-L/Sentiment-Analysis :

- subj_pos_multigrams.csv,
- subj_pos_unigrams.csv,
- subj_neg_multigrams.csv,
- subj_neg_unigrams.csv.

Here are a few examples from each file of polarized n-grams.

Positive sentiment oriented unigrams from subj_pos_unigrams.csv (stemmed):
"super", "awesom", etc. 

Some positive multigrams from the file sub_pos_multigrams.csv (not stemmed): "no trouble", "5 stars", "thumbs up", "it's a ten", "as described", "know what they're doing". 

Possible variants have usually been added, including variants originating from spelling errors or "alternative" grammar: "no troubles", "not any trouble", "not any troubles", "no problem", "no problems", etc.; "five stars", "five star", "5-star", "5star", 5 star"; "must have"; "it's a 10", "it's a ten", "its a 10", etc.; 
"know what theyre doing", "know what they are doing", etc. 

Some negative unigrams (after stemming) from the file subj_neg_unigrams.csv: "horribl", "crap", "whine", etc.

Some negative multigrams (not stemmed) from the file sub_neg_multigrams.csv: "1 star", "one star", "not good", "no good", "shouldn't" (often associated with negative context), "pretty piece of junk", etc.

Some efficacy-minded rules will be applied. 

First, the polarized n-grams will be preceded and followed by one white space character when looking for occurrences in reviews. Otherwise, in the bag of words, the n-gram "most inconvi" would become "most in subjpo " (because "convi" is a polarized unigram in subj_pos_unigrams.csv) and then "  subjpo " (because "most" and "in" are
stopwords in stopwords_without_apostrophe.csv)! A negatively oriented multigram would become a positively oriented unigram! Consequently, one white space character is added in front of and at the end of each polarized n-gram before looking for matching occurrences in NLP-transformed reviews, in order to avoid replacing substrings.

Second, as a consequence, a white space character has to be added at the beginning and at the end of each NLP-transformed review! Otherwise, polarized n-grams, which are preceded and followed by one white space character can never match an occurrence that is positioned at the beginning or at the end of a review. 

Third, as already indicated, " subpos " and "subjneg " contain one white space character at the beginning and at the end, in order to prevent amalgamation. Indeed, what would happen if white space characters were not added? Let's take our well known example of " conveni ": if it were replaced with just "subjpo" in the n-gram " most conveni ", then it would produce " mostsubjpo", which would no longer be a generic positive unigram! Transformation would be useless if not annoyingly counterproductive! 

Fourth, multiple inter word white space characters have to be reduced to a single inter word white space character: indeed two multigrams differing only in the number(s) of inter word white space characters are treated as different multigrams. 

Fifth, in reviews, matching negative n-grams have got to be replaced before matching positive ones. Let's take the example of " poor fit ", which is a negatively polarized multigram in the file subj_neg_multigrams.csv: if matching with occurrences in reviews begins with positively polarized n-grams, then " poor fit " becomes " poor subjpo ", with " poor " possibly not counterbalancing " subpos " if it is not in a node of the decision tree (or if it is at too low a level).

Sixth, among positive or negative polarized multigrams, they should be tentatively matched in decreasing order in for loops. Why? Let's take an example of " no good bargain " in one review. In sub_neg_multigrams.csv, there are two negatively polarized multigrams: " no good bargain " and " no good"; if these are considered in 
decreasing order, then, in the review, " no good bargain " is replaced with " subjneg ", which looks good; otherwise "no good bargain " is replaced with " subjneg bargain " and then " subjneg subjpo ": consequently, instead of having one negative generic 
unigram we would get one positive and one negative generic unigrams!

NLP will be rerun again. In each training review, all n-grams that match positive n-grams from subj_pos_multigrams.csv or subj_pos_unigrams.csv will be replaced with a generic positive token ("subjpo"); all n-grams that match negative n-grams from subj_neg_multigrams.csv or subj_neg_unigrams.csv will be replaced with a generic negative token ("subjneg"). 

The utf8 package will be used to normalize punctuation: there has been some trouble with curly apostrophes instead of straight apostrophes.

<br>

```{r Polarizing and rerunning CART}
myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/subj_pos_multigrams.csv"
subj_pos_multigrams <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
subj_pos_multigrams <- sort(subj_pos_multigrams[, 2], decreasing = TRUE) %>%                                as.vector()
# Converting curly apostrophes to straight apostrophes. 
subj_pos_multigrams <- sapply(subj_pos_multigrams, utf8_normalize, map_quote = TRUE)
subj_pos_multigrams <- paste("", subj_pos_multigrams, "")

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/subj_pos_unigrams.csv"
subj_pos_unigrams <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
subj_pos_unigrams <- subj_pos_unigrams[, 2] %>% as.vector()
subj_pos_unigrams <- paste("", subj_pos_unigrams, "")

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/subj_neg_multigrams.csv"
subj_neg_multigrams <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
subj_neg_multigrams <- sort(subj_neg_multigrams[, 2], decreasing = TRUE) %>%                                as.vector()
# Converting curly apostrophes to straight apostrophes. 
subj_neg_multigrams <- sapply(subj_neg_multigrams, utf8_normalize, map_quote = TRUE)
subj_neg_multigrams <- paste("", subj_neg_multigrams, "")

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/subj_neg_unigrams.csv"
subj_neg_unigrams <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
subj_neg_unigrams <- subj_neg_unigrams[, 2] %>% as.vector()
subj_neg_unigrams <- paste("", subj_neg_unigrams, "")
rm(myfile)

# Creating and lowercasing corpus.
corpus_av2 <- VCorpus(VectorSource(reviews_train$text)) 
corpus_av2 <- tm_map(corpus_av2, content_transformer(tolower))

# Replacing all punctuation marks by spaces except for apostrophes and hyphens. 
for (i in 1:nrow(reviews_train)) {
  corpus_av2[[i]]$content <- 
    gsub("[.?!]", " ", gsub("(?![-.?!'])[[:punct:]]", " ", 
                            corpus_av2[[i]]$content, perl=T))
}

# Removing spaces at the beginning and at the end of reviews
# to get apostrophes in first or last position if they are at 
# the beginning or at the end of a review.
for (i in 1:nrow(reviews_train)) {
  corpus_av2[[i]]$content <- trimws(corpus_av2[[i]]$content, which = "l")
  corpus_av2[[i]]$content <- trimws(corpus_av2[[i]]$content, which = "r")
}

# Removing apostrophes and hyphens at the beginning and at the end of reviews,
# with repetition (in case there are several of them).
for (i in 1:nrow(reviews_train)) {
  for (j in 1:12) {
    corpus_av2[[i]]$content <- sub("^[[:punct:]]","", corpus_av2[[i]]$content)
    corpus_av2[[i]]$content <- sub("[[:punct:]]$","", corpus_av2[[i]]$content)
  }
}

# Adding one space at the beginning and at the end of reviews.
for (i in 1:nrow(reviews_train)) {
  corpus_av2[[i]]$content <- paste("", corpus_av2[[i]]$content, "") 
}

# Reducing interword white space to one single character. 
corpus_av2 <- tm_map(corpus_av2, stripWhitespace)

# Matching multigrams from reviews with polarized multigrams 
# from subj_neg_multigrams.csv or subj_pos_multigrams.csv.
# If matching works, replacing multigrams from reviews 
# with generic polarized unigram " subjpo " or " subneg ".
for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_neg_multigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_neg_multigrams[j], " subjneg ", 
                                    corpus_av2[[i]]$content)
  }
}

for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_pos_multigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_pos_multigrams[j], " subjpo ", 
                                    corpus_av2[[i]]$content)
  }
}

# Removing short forms.
corpus_av2 <- tm_map(corpus_av2, removeWords, short_forms_neg)
corpus_av2 <- tm_map(corpus_av2, removeWords, short_forms_pos)

# Replacing each remaining apostrophe or hyphen with one single white space 
# character. 
for (i in 1:nrow(reviews_train)) {
  corpus_av2[[i]]$content <- gsub("[[:punct:]]", " ", corpus_av2[[i]]$content)
}

# Polarizing multigrams again (some apostrophes or hyphens 
# might have prevented taking some n-grams into account).
for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_neg_multigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_neg_multigrams[j], " subjneg ", 
                                    corpus_av2[[i]]$content)
  }
}

for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_pos_multigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_pos_multigrams[j], " subjpo ", 
                                    corpus_av2[[i]]$content)
  }
}

# Stemming reviews.
corpus_av2 <- tm_map(corpus_av2, stemDocument)

# Challenge: the function stemDocument suppresses spaces at the beginning 
# and at the end of each review. Consequently, one space has to be added again
# at the beginning and at the end of each review.

for (i in 1:nrow(reviews_train)) {
  corpus_av2[[i]]$content <- paste("", corpus_av2[[i]]$content, "") 
}

# Polarizing multigrams again after stemming. Some multigrams might have 
# become eligible after stemming. 
for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_neg_multigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_neg_multigrams[j], " subjneg ", 
                                    corpus_av2[[i]]$content)
  }
}

for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_pos_multigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_pos_multigrams[j], " subjpo ", 
                                    corpus_av2[[i]]$content)
  }
}

# Matching polarized unigrams with unigrams in reviews and,
# if it is the case, replacing matching unigrams from reviews 
# with a generic polarized unigram. 
for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_neg_unigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_neg_unigrams[j], " subjneg ", 
                                    corpus_av2[[i]]$content)
  }
}

for (i in 1:nrow(reviews_train)) {
  for (j in 1:length(subj_pos_unigrams)) {
    corpus_av2[[i]]$content <- gsub(subj_pos_unigrams[j], " subjpo ", 
                                    corpus_av2[[i]]$content)
  }
}

# Removing remaining stopwords.
corpus_av2 <- tm_map(corpus_av2, removeWords, stopwords_remaining)

# Removing numbers, digits and extra white space characters.
corpus_av2 <- tm_map(corpus_av2, removeNumbers)
corpus_av2 <- tm_map(corpus_av2, stripWhitespace)

# Creating document term matrix, handling sparsity, converting 
# to data frame, making Colnames R-friendly and adding independent variable. 
dtm_av2 <- DocumentTermMatrix(corpus_av2)
sparse_av2 <- removeSparseTerms(dtm_av2, 0.995)
sentSparse_av2 <- as.data.frame(as.matrix(sparse_av2)) 
rownames(sentSparse_av2) <- 1:nrow(sentSparse_av2)
colnames(sentSparse_av2) <- make.names(colnames(sentSparse_av2))
sentSparse_av2 <- sentSparse_av2 %>% mutate(sentiment = reviews_train$sentiment)

# Building a CART model with tuning.
set.seed(1)
fit_cart_tuned_av2 <- train(sentiment ~ .,
                            method = "rpart",
                            data = sentSparse_av2,
                            tuneLength = 15, 
                            metric = "Accuracy")
fitted_cart_tuned_av2 <- predict(fit_cart_tuned_av2)
cm_cart_tuned_av2 <- 
  confusionMatrix(as.factor(fitted_cart_tuned_av2), 
                  as.factor(sentSparse_av2$sentiment))

# Printing accuracy level.
tab <- data.frame(cm_cart_tuned_av2$overall["Accuracy"]) %>%
       `rownames<-`(
          'Model: Negation + Polarization + CART + Tuning') %>%
       `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(tab, "html", align = "c") %>% 
       kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
       column_spec(1, bold = T, color = "#808080") %>%
       column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2")
rm(tab)
```

<br>

Polarizing n-grams that convey some subjective information has dramatically boosted accuracy from 81 % to 92 %. Nevertheless, let us stay realistic: text mining has been performed on the training set and is not necessarily or at least not necessarily entirely transposable to the validation set. 

There is probably an earthquake in both the wordcloud and the decision tree. 

```{r Expanding display width in anticipation of wordcloud}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Wordcloud from bag of words with negation and polarized n-grams}
df <- sentSparse_av2 %>% select(- sentiment)
wordcloud(colnames(df), colSums(df), min.freq = 10, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
rm(df)
```

```{r shrinking display width back again to 60 %}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

The most frequent tokens are now "subjpo" and "subjneg". "not" is present, at the "fourth level" (graphically). 

The histogram.

<br>

```{r Histogram from tuned CART after polarizing}
df <- sentSparse_av2 %>% select(- ncol(.))
freq <- data.frame(to = colnames(df), fre = as.integer(colSums(df)), 
                   stringsAsFactors = FALSE) %>% 
        arrange(desc(fre)) %>% head(., 12)
graph <-  freq %>% mutate(to = reorder(to, fre)) %>%
  ggplot(aes(to, fre)) + 
  geom_bar(stat = "identity", width = 0.80, color = "#007ba7", fill = "#007ba7") + 
  coord_flip() +
  ggtitle("Token Frequency") +
  xlab("Token") + ylab("Frequency") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

```{r Expanding display width for decision tree}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Decision tree from tuned CART after polarizing}
prp(fit_cart_tuned_av2$finalModel, uniform = TRUE, cex = 0.6, 
    box.palette = "-auto")
```

```{r Reducing display width back again to 60 % for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br>

Now, the first node is occupied by "subjneg". Then comes "subjpo" and at the same level "not" and "great". The two generic tokens, with polarized sentiment orientation, are in the top nodes of the tree. "not" is also highly ranked. Many individual tokens that were previously in nodes of the tree, have disappeared.

The next table summarizes results obtained so far. 

<br>

```{r Result table}
colname <- c("MODEL ID", "SHORT DESCRIPTION", "ACCURACY", "SENSITIVITY", 
             "NEG PRED VAL", "SPECIFICITY", "POS PRED VAL")
models <- c("cart_tuned_av1_a", "cart_tuned_av1_b", "cart_tuned_av1_c",
            "cart_tuned_av2")
description <- c("CART + tuning + negation",
                 "CART + tuning + negation + neg short forms",
                 'CART + tuning + negation + neg short forms = "not"',
                 "CART + tuning + negation + polarization")
cm <- c("cm_cart_tuned_av1_a", "cm_cart_tuned_av1_b", "cm_cart_tuned_av1_c",
        "cm_cart_tuned_av2")
tab <- data.frame(matrix(1:(length(colname) * length(models)),
                         ncol = length(colname), nrow = length(models)) * 1)

for (i in 1:length(models)) {
  tab[i, 1] <- models[i]
  tab[i, 2] <- description[i]
  tab[i, 3] <- 
    eval(parse(text = paste(cm[i], "$overall['Accuracy']", sep = "")))
  tab[i, 4] <- 
    eval(parse(text = paste(cm[i], "$byClass['Sensitivity']", sep = "")))
  tab[i, 5] <- 
    eval(parse(text = paste(cm[i], "$byClass['Neg Pred Value']", sep = "")))
  tab[i, 6] <- 
    eval(parse(text = paste(cm[i], "$byClass['Specificity']", sep = "")))
  tab[i, 7] <- 
    eval(parse(text = paste(cm[i], "$byClass['Pos Pred Value']", sep = "")))
}                 

tab_av_1_2 <- tab %>% mutate_at(vars(3:7), funs(round(., 4))) %>%
           `colnames<-`(colname)

# Recalling previous table and making sure the colnames are regularized. 
tab <- tab_av0 %>% `colnames<-`(colname)

# Printing.
tab_av_0_1_2 <- rbind(tab, tab_av_1_2)
knitr::kable(tab_av_0_1_2, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  row_spec(c(1, 5:6),  bold = T, strikeout = T, 
           color = "#08457E", background = "#9bc4e2") %>%
  row_spec(2:4, bold = T, color = "#08457E", background = "#9bc4e2") %>%
  row_spec(7, bold = T, color = "#205030", background = "#a7e3bb")

rm(models, description, cm, tab)
rm(tab_av0, tab_av_1_2)
```

<br>

In the table above, on rows 1 and 4, fonts have been blurred into light blue and have been stricken through to indicate that these models have been discarded. The other four models should be seen as a cumulative process bringing accuracy improvement in a stepwise and incremental way. 

As shown in the last two rows, thanks to polarization, accuracy has jumped from 81 % up to 92 %, which is impressive. More impressive: sensitivity has sprung from 66 % to 89 %. 

False negatives have been a recurrent weak point in machine learning results up to now. But special attention has been paid to them in debriefing previous machine learning results and in text classification of subjective information n-grams figuring in false negatives and not in decision trees, i.e. in classifying positively oriented n-grams as " subjpo ". Some attention has also been paid to false positives,
less numerous though. 

Let's have a look at the numbers of remaining false negatives and positives in the following confusion matrix.

<br>

```{r Confusion table from tuned CART after polarizing}
tab <- table(fitted_cart_tuned_av2, sentSparse_av2$sentiment) %>% as.vector()
tab <- data.frame(matrix(tab, ncol = 2, nrow = 2, byrow = FALSE)) %>%
  `colnames<-`(c("Actually positive", "Actually negative")) %>%
  `rownames<-`(c("Predicted positive AFTER POLARIZING", 
                 "Predicted negative AFTER POLARIZING"))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "#808080") %>%
  column_spec(2, bold = T, color = "#08457E", background = "#9bc4e2") %>%
  column_spec(3, bold = T, color = "#205030", background = "#a7e3bb") 

# sentSparse_av2 will be preserved for further use in the part about machine learning.
rm(reviews_train)
rm(corpus_av1_a, dtm_av1_a, sparse_av1_a)
rm(fit_cart_tuned_av1_a, fitted_cart_tuned_av1_a, cm_cart_tuned_av1_a)
rm(corpus_av1_b, dtm_av1_b, sparse_av1_b, sentSparse_av1_b)
rm(fit_cart_tuned_av1_b, fitted_cart_tuned_av1_b, cm_cart_tuned_av1_b)
rm(corpus_av1_c, dtm_av1_c, sparse_av1_c, sentSparse_av1_c)
rm(fit_cart_tuned_av1_c, fitted_cart_tuned_av1_c, cm_cart_tuned_av1_c)
rm(corpus_av2, dtm_av2, sparse_av2)
rm(fit_cart_tuned_av2, fitted_cart_tuned_av2, cm_cart_tuned_av2)
rm(df, freq, i, j, colname, tab, tab_av_0_1_2)
```

<br>

At the previous step, results were less good, especially for false negatives, as already shown in a table. The number of false negatives has been crushed from 114 to 38; parallelwise, the number of true negatives has climbed from 220 to 296. The decrease in false positives is much less impressive, from 22 to 17. 

After improving accuracy thanks to NLP and text mining in the previous sections, new accuracy improvements will be looked for in the next section through machine learning, diversifying models. 

<br>

## IV. MACHINE LEARNING SEARCH FOR ACCURACY

### A. Constructing Validation Set

<br>

Ten models are going to be applied.

Because of high overfitting risk with some models, comparing models on accuracy performance will probably require more tools than just accuracy on training set. 

Before, the corpus, and consequently the training set, were built only on the training rows (ind_train). It was to be on the safe side: frequencies from the validation rows could not impact the selection of tokens when applying the sparsity threshold. Insulation prevented cross-effects, probably very marginal though.

Here, we have to work on the whole data set reviews to have the same number of columns in the training set and in the validation set. To remain on the safe side,  the possible impact of these cross-effects will be measured by comparing results from CART already obtained and new results from CART.

The bag of words will be prepared in the same way as in the previous section:
- building up the corpus, 
- lowercasing,
- punctuation handling,
- white space handling,
- removing negative and positive short forms, 
- keeping negation unigrams such as "not",
- searching for corpus n-grams that match listed negative n-grams and replacing them with " subjneg ",
- doing the equivalent for positive n-grams, replacing them with " subjpo ",
- stemming, 
- removing remaining stopwords,
- removing numbers,
- tokenizing,
- building up Document Term Matrix and data frame and adding dependent variable.

The resulting data frame will be split according to previous indexes into training set and validation set. 

<br>

```{r Validation set for machine learning}
# Training set already exists, it is sentSparse_av2. 
# Model on training set also exists, it is lit_cart_tuned_av2. 

# Validation bag of words has to be created. 
# Selecting validation reviews_val with previously defined ind_val.
reviews_val <- reviews[ind_val, ]

# Creating and lowercasing corpus.
corpus <- VCorpus(VectorSource(reviews_val$text)) 
corpus <- tm_map(corpus, content_transformer(tolower))

# Replacing all punctuation marks by white space characters space except for apostrophes and hyphens. 
for (i in 1:nrow(reviews_val)) {
  corpus[[i]]$content <- 
    gsub("[.?!]", " ", gsub("(?![-.?!'])[[:punct:]]", " ", 
                            corpus[[i]]$content, perl=T))
}

# Removing spaces at the beginning and at the end of reviews_val.
for (i in 1:nrow(reviews_val)) {
  corpus[[i]]$content <- trimws(corpus[[i]]$content, which = "l")
  corpus[[i]]$content <- trimws(corpus[[i]]$content, which = "r")
}

# Removing apostrophes and hyphens at the beginning 
# and at the end of reviews_val, with repetition. 
for (i in 1:nrow(reviews_val)) {
  for (j in 1:12) {
    corpus[[i]]$content <- sub("^[[:punct:]]","", corpus[[i]]$content)
    corpus[[i]]$content <- sub("[[:punct:]]$","", corpus[[i]]$content)
  }
}

# Adding one space at the beginning and at the end of reviews_val.
for (i in 1:nrow(reviews_val)) {
  corpus[[i]]$content <- paste("", corpus[[i]]$content, "") 
}

# Reducing multiple white space characters to one: otherwise, a multigram 
# with multiple inter-word white space characters can match a multigram
# neither from subj_pos_multigrams.csv nor from subj_neg_multigrams.csv. 
corpus <- tm_map(corpus, stripWhitespace)

# Polarizing review multigrams by substitution.
for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_neg_multigrams)) {
    corpus[[i]]$content <- gsub(subj_neg_multigrams[j], " subjneg ", 
                                corpus[[i]]$content)
  }
}

for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_pos_multigrams)) {
    corpus[[i]]$content <- gsub(subj_pos_multigrams[j], " subjpo ", 
                                corpus[[i]]$content)
  }
}

# Removing short forms.
corpus <- tm_map(corpus, removeWords, short_forms_neg)
corpus <- tm_map(corpus, removeWords, short_forms_pos)

# Replacing all (possibly remaining) apostrophes and hyphens with spaces. 
for (i in 1:nrow(reviews_val)) {
  corpus[[i]]$content <- gsub("[[:punct:]]", " ", corpus[[i]]$content)
}

# Polarizing multigrams again: hyphens 
# might have prevented polarizing some strings.
for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_neg_multigrams)) {
    corpus[[i]]$content <- gsub(subj_neg_multigrams[j], " subjneg ", 
                                corpus[[i]]$content)
  }
}

for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_pos_multigrams)) {
    corpus[[i]]$content <- gsub(subj_pos_multigrams[j], " subjpo ", 
                                corpus[[i]]$content)
  }
}

# Stemming reviews_val.
corpus <- tm_map(corpus, stemDocument)

# Challenge: the function stemDocument suppresses spaces at the beginning 
# and at the end of each review. Consequently, one space has to be added again
# at the beginning and at the end of each review.
for (i in 1:nrow(reviews_val)) {
  corpus[[i]]$content <- paste("", corpus[[i]]$content, "") 
}

# Polarizing multigrams again after stemming. Some multigrams migh have 
# become eligible after stemming. 
for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_neg_multigrams)) {
    corpus[[i]]$content <- gsub(subj_neg_multigrams[j], " subjneg ", 
                                corpus[[i]]$content)
  }
}

for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_pos_multigrams)) {
    corpus[[i]]$content <- gsub(subj_pos_multigrams[j], " subjpo ", 
                                corpus[[i]]$content)
  }
}

# Polarizing unigrams by substitution.
for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_neg_unigrams)) {
    corpus[[i]]$content <- gsub(subj_neg_unigrams[j], " subjneg ", 
                                corpus[[i]]$content)
  }
}

for (i in 1:nrow(reviews_val)) {
  for (j in 1:length(subj_pos_unigrams)) {
    corpus[[i]]$content <- gsub(subj_pos_unigrams[j], " subjpo ", 
                                corpus[[i]]$content)
  }
}

# Removing stopwords, numbers, digits and extra white space characters.
corpus <- tm_map(corpus, removeWords, stopwords_remaining)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)

# Creating Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# No sparsity threshold is applied, since this could discard
# tokens present in the separately built training set. 

# Converting to data frame, making colnames R-friendly 
# and adding dependent variable. 
sentSparse <- as.data.frame(as.matrix(dtm)) 
colnames(sentSparse) <- make.names(colnames(sentSparse))
sentSparse <- sentSparse %>% 
  mutate(sentiment = reviews_val$sentiment) %>% as.data.frame()
val <- sentSparse

# Reinstating former training set. 
train <- sentSparse_av2
df <- train[, -ncol(train)]
v <- colSums(df)
l <- which(v > 10)
train <- df[ , l] %>% mutate(sentiment = sentSparse_av2$sentiment) %>%
  as.data.frame()

# For machine learning, columns have to match between training set 
# and test set: adjustments have to be made on the validation set.
# Let's keep only colums that alo exist in the training set. 
# The column "sentiment" will remain since the name exists in both sets.
val <- val %>% as.data.frame() %>% 
               select(intersect(colnames(.), colnames(train)))

# Columns from train that are missing in val have to be added as null vectors.
mis <- setdiff(colnames(train), colnames(val))
df <- data.frame(matrix((nrow(val) * length(mis)), 
                        nrow = nrow(val), ncol = length(mis)) * 0) %>%
      `colnames<-`(mis)
val <- cbind(val, df) %>% as.data.frame()

      set.seed(1)
      fit_essai <- train(sentiment ~ ., 
                   method = "xgbLinear", tuneLength = 15,
                   data = train, metric = "Accuracy")
      fitted_essai <- predict(fit_essai)
      mean(fitted_essai == train$sentiment)
      pred <- predict(fit_essai, newdata = val)
      mean(pred == val$sentiment)

rm(subj_neg_multigrams, subj_neg_unigrams, 
   subj_pos_multigrams, subj_pos_unigrams)
rm(corpus, dtm, sentSparse)
rm(i, j)
```

<br>

### B. Machine Learning Models

<br>

Ten machine learning models have been chosen. Here is the list, with an identifier and a short description for each model. 

<br>

```{r Identification and description of the 10 machine learning models} 
IDs <- c("adaboost_03", "cart_03", "gbm_03", "monmlp_03", "rf_03", 
         "svm_03", "xgb_03", "cart_15", "gbm_15", "svm_15")
models <- c("AdaBoost Classification Trees",
            "CART",
            "Stochastic Gradient Boosting",
            "Monotone Multi-Layer Perceptron Neural Network",
            "Random Forest",
            "Support Vector Machines with Radial Basis Function Kernel",
            "eXtreme Gradient Boosting",
            "CART",
            "Stochastic Gradient Boosting",
            "Support Vector Machines with Radial Basis Function Kernel")
caret_names <- c("adaboost", "rpart", "gbm", "monmlp", "rf", "svmRadialCost",                         "xgbLinear", "rpart", "gbm", "svmRadialCost")
tunings <- c(rep(3, 7), rep(15, 3))
nr_resamples <- rep(25, 10)

# The colnames of the model presentation table are:
colname_methods <- c("MODEL ID", "MODEL", "NAME IN CARET", 
                    "# TUNING VALUES", "# BOOTSTRAPPED RESAMPLES")

# Building up the presentation table of models and printing it.
tab <- data.frame(IDs, models, caret_names, tunings, nr_resamples) %>%
       `colnames<-`(colname_methods)
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  row_spec(1:7,  bold = T, 
           color = "#08457E", background = "#9bc4e2") %>%
  row_spec(8:10, bold = T, color = "white", background = "#08457E") 
rm(tab)
```

<br> 

All models will be trained with the train() function from the package caret. 

For four models, by default, training is done on 25 bootstrapped resamples and on 3 values of the parameters tuned. These models are "adaboost", "rf", "xgb" and "monmlp". For three models that run faster on my PC, two options have been applied: either default training or training with tuning on 15 values across parameters tuned.

The names of the parameters tuned by the train() function are available in  http://topepo.github.io/caret/available-models.html.

<br>

### C. Accuracy Results on the Training Set

<br>

The accuracy results are summarized in the next table. 

<br>

```{r Training 10 machine learning models}
# Identifying separately models with standard tuning and models with extra tuning.
rows_standard_tuning <- 1:7
rows_extra_tuning <- setdiff(1:length(IDs), rows_standard_tuning)
IDs_standard_tuning <- IDs[rows_standard_tuning]
IDs_extra_tuning <- IDs[rows_extra_tuning]
caret_names_standard_tuning <- caret_names[rows_standard_tuning]
caret_names_extra_tuning <- caret_names[rows_extra_tuning]

# Running models with standard tuning.
# The function capture.output prevents models gbm and monmlp 
# producing verbose output.
fits_standard_tuning <- list(1)
for (i in 1:length(caret_names_standard_tuning)) {
    dustbin <- capture.output({
      set.seed(1)
      fits_standard_tuning[[i]] <- train(sentiment ~ ., 
                   method = caret_names_standard_tuning[i], 
                   data = train, metric = "Accuracy")
      })
} 
rm(dustbin)

# Running models with extra tuning.
fits_extra_tuning <- list(1)
for (i in 1:length(caret_names_extra_tuning)) {
    dustbin <- capture.output({
      set.seed(1)
      fits_extra_tuning[[i]] <- train(sentiment ~ ., 
                   method = caret_names_extra_tuning[i], data = train, 
                   tuneLength = 15,  metric = "Accuracy")
      })
}
rm(dustbin)

# Naming result bulks. 
names(fits_standard_tuning) <- IDs_standard_tuning
names(fits_extra_tuning) <- IDs_extra_tuning

# Regrouping result bulks into one list.
fits <- append(fits_standard_tuning, fits_extra_tuning)

# Ordering that list.
fits <- fits[IDs]

# Getting predictions on training set.
df <- data.frame(matrix(1:(nrow(train) * length(fits)), 
                        ncol = length(fits), nrow = nrow(train)) * 1)
for (i in 1:length(fits)) {
  df[, i] <- predict(fits[[i]])
}

# Using predictions on the training set to compute accuracy for each model.
tab <- data.frame(matrix(1:(length(fits)), ncol = 1, nrow = length(fits)))
for (i in 1:length(fits)) {
  tab[i, 1] <- mean(df[, i] == train$sentiment)
}                    

# Preparing column names for result table.
colname_results <- c("MODEL ID", "MODEL", "# TUNING VALUES", 
                     "# BOOTSTRAPPED RESAMPLES", "ACCURACY ON THE TRAINING SET")

# Building up result table and printing it. 
tab <- tab %>% 
  `colnames<-`("acc") %>%
  mutate(acc = round(acc, 4)) %>%
  mutate(ID = IDs) %>% 
  mutate(mod = models) %>%
  mutate(tuning = tunings) %>%
  mutate(boot = nr_resamples) %>%
  select(ID, mod, tuning, boot, acc) %>% 
  arrange(desc(acc)) %>%
  `colnames<-`(colname_results)

knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  row_spec(1:length(fits), bold = T, color = "#08457E", background = "#9bc4e2")
```

<br>

Not surprisingly, most models seem to overfit. As pointed out in literature, it is a rather widespread tendency, the more so with complex algorithms which have many parameters they can minimize the loss function on. 

Moreover, NLP and text mining have been extensively adjusted on the training set. 

Consequently, the ranking figuring in the table above is no appropriate tool to separate models. 

Accuracy cannot be tested on the validation set since the validation set is only to be used as a last step; if some testing is performed on the validation set, it is no longer a validation set. 

Moreover, the whole sample being very limited (1,000 reviews), it was  probably potentially counter-productive to divide it into training, test and validation sets. 

<br>

### D. Testing on Bootstrap Resample Accuracy Distributions

<br>

But there are some unused resources. The train() function from caret has automatically trained the models on 25 bootstrapped resamples, for all values of the tuned parameters, i.e. on three values of the tuned parameters except where extra tuning on 15 values had been asked, i.e. CART, svm and gbm (chosen because running time was limited). Actually, there is tuning on all models but in three cases there is extra tuning. 

For each of the 10 models, accuracy is available for each of the 25 resamples: for each model and for each resample it is accuracy with the parameter optimizing value(s), i.e. the value(s) that deliver(s) the highest accuracy level on average on all the 25 resamples. 

This produces for each model a distribution of accuracy, i.e. a set of 25 accuracy levels corresponding to the optimizing parameter value(s).

Each resample is a sample of the training set with replacement. It has the same number of rows as the training set. That is why the bootstrapping method has been preferred to e.g. the K-fold cross-validation, which would have implied further splitting of an already small dataset. 

For each model, the bootstrapped accuracy distribution can deliver precious indications: what's the average, the median, the maximum? It can tell about the generalizability of the accuracy levels computed. 

First, accuracy distributions will be extracted for the 10 models. Ten graphs will be drawn with the resample accuracy distribution from the ten models.

<br>

```{r Adapting opts_chunk to enlarge display width for a set of graphs}
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Bootstapped accuracy distributions}
# The list of model IDs has already been defined under the name "IDs". 
distributions <- 
  data.frame(matrix(1:(length(fits) * nr_resamples[1]), 
                    ncol = length(fits), nrow = nr_resamples[1]) * 1) %>%
  `colnames<-`(IDs) 
for (i in 1:length(fits)) {
  expressio <- paste("fits$", IDs[i], "$resample$Accuracy", sep = "")
  distributions[, i] <- eval(parse(text = expressio))
}

l <- list(1:10)
for (i in 1:length(fits)) {
  graph <- distributions %>% select(i) %>% as.data.frame() %>% 
    `colnames<-`("dist") %>%
    ggplot(aes(dist)) + 
    geom_histogram(bins = 7, color = "#08457E", fill = "#08457E") + 
    geom_vline(aes(xintercept = mean(dist)), col = "magenta", size = 2) +
    geom_vline(aes(xintercept = median(dist)), col = "yellow", 
               linetype = "dashed", size = 2) +
    ggtitle(IDs[i]) +
    theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
          axis.title.x = element_blank(), axis.title.y = element_blank(), 
          axis.text.x = element_text(size = 12), 
          axis.text.y =element_text(size = 12))
  l[[i]] <- graph
}

marrangeGrob(l, nrow = 5, ncol = 2, 
  top = quote(paste("BOOTSTRAP RESAMPLE ACCURACY DISTRIBUTION PER MODEL")))
```

```{r Reducing display width back to 60 % for other figures}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br> 

On these graphs, the mean appears as a vertical magenta line and the median as a vertical dashed yellow line. 

What's new in comparison with information delivered by predicting on the whole training set? 

First, levels are lower even if means and medians are above 90 % for most models, with the exceptions of cart and monmlp which are thereby loosing chances to be picked for validation on the validation set.

For each model, even the maximum resampled accuracy is lower than the training set accuracy. The average accuracy is still lower. The sharpest fall can be seen for the svm and svm_tuned models.

Expressed in means, differences between most models are more limited. The svm_tuned model, which outperformed the other models on the whole training set, has almost been caught up by xgb in average and has even been passed in median. 

What can be seen as a serious handicap for some models is a left-skewed distribution, even if it is only due to 2 resamples. A long left tail is problematic from a risk management viewpoint since some resamples show accuracy levels that are  substantially lower than the average and the median, even if these resamples are a minority. It's the case for monmlp (already out) but also for the highly performing svm and svm_tuned at least in average and median. Thus svm and svm_tuned will also be excluded from the validation process. This criterion will even be generalized: models will be ranked on the basis of their accuracy minimum and the top 3 will be combined into an ensemble model. 

To evaluate the merits of the remaining models, let's produce a comparative table. 

<br>

```{r Comparative table of bootstapped accuracy distributions}
df <- data.frame(matrix(1:(length(fits) * 6), nrow = length(fits), ncol = 6) * 1) %>%
  `colnames<-`(as.character(names(summary(distributions[[1]])))) %>%
  mutate(ID = IDs) %>% select(ID, everything())

for (i in 1:(length(fits))) {
  df[i, 2:7] <- round(as.numeric(matrix(summary(distributions[[i]]))), 4)
}

# Ordering df on the basis of the minimum of resampled accuracy.
df <- df %>% arrange(- Min.)

kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(2:10, bold = T, color = "#08457E", background = "#9bc4e2")

rm(tab, df, l, graph, colname_methods, colname_results, expressio)
rm(rows_extra_tuning, rows_standard_tuning)
rm(IDs, IDs_extra_tuning, IDs_standard_tuning)
rm(caret_names, caret_names_extra_tuning, caret_names_standard_tuning)
rm(models, tunings, nr_resamples)
rm(fits_extra_tuning, fits_standard_tuning, distributions)
```

<br> 

The top model is xgb. 

For the record, cart-tuned doesn't score badly: it is rather close to gbm_tuned in minimum; it had been used as a practical yardstick in the NLP and text mining part, running relatively quickly and delivering precise information about decision trees.

For the record again, cart-tuned delivers the same accuracy level as when the bag of word was established separately for the train set as in the NLP and text mining sections, to avoid the slightest risk of cross-influence of word frequencies. In this machine learning section, to have the same number of columns, a common bag of words has been developed before splitting into the training set and the validation set: this has exercised no influence on the training set accuracy produced by cart_tuned.

<br>

### E. Validation on the Validation Set

<br>

The xgb model can now be validated on the validation set: predictions will be computed on the validation set and then accuracy. 

<br>

```{r Validation}
# Predictions on the validation set with xgb (and CART for further use)
pred_xgb_03 <- predict(fits$xgb_03, newdata = val)
pred_cart_15 <- predict(fits$cart_15, newdata = val)

# Accuracy on the validation set with xgb (and CART)
acc_xgb_03 <- round(mean(pred_xgb_03 == val$sentiment), 4)
acc_cart_15 <- round(mean(pred_cart_15 == val$sentiment), 4)

# Accuracy on the validation set with baseline model
ref <- as.character(val$sentiment)
pred_baseline <- data.frame(sentiment = rep(" Pos", nrow(val)),
                            stringsAsFactors = FALSE)
acc_baseline <- sprintf("%.4f", 
                round(mean(pred_baseline$sentiment == val$sentiment), 4)) 

# Printing accuracy on the validation set with baseline model and xgb.
tab <- data.frame(matrix(c(acc_baseline, acc_xgb), nrow = 2, ncol = 1)) %>%
  `colnames<-`("ACCURACY ON THE VALIDATION SET") %>% 
  `rownames<-`(c("Baseline Model", 
                 "Final Model: NLP + Text Mining + XGBoost + Default Tuning"))

kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(2, bold = T, color = "#205030", background = "#a7e3bb") %>%
  row_spec(1, bold = T, color = "#08457E", background = "#9bc4e2")  

############## Remettre pour supprimer fits.

rm(val, pred_xgb, pred_baseline, pred_cart_tuned, ref, tab)
```

<br>

## V. TAKING STOCK OF METHODOLOGIES AND RESULTS

### A. Methodology Summary

<br>

Through NLP, words have been standardized, sentences (reviews) have been tokenized into stemmed tokens, more frequent tokens have been assembled into an actionable Document Term Matrix linking reviews, tokens and labels, i.e. true sentiment orientations of reviews. At the end of this process, prediction gave 79 % accuracy on the training set with a tuned CART model, against only 50 % with a baseline model ignoring NLP. 

Through customized text mining, usable subjective information has been retrieved. Comparing token frequency and decision trees has given one insight: the importance of words conveying subjective information. Perusing random samples of false negatives and false positives has produced two insights: first, in many false negatives and false positives, there is clear subjective information available but that does not show in decision trees; second, sentiment polarity of reviews is inverted by negation in a non negligible number of false negatives and false positives. The second insight has led to integrating negation unigrams such as "not". The first insight has conducted to establishing customized lists of positively oriented tokens and negatively oriented tokens and to replacing them in all reviews with a generic positive token or a generic negative token. 

Performance boosting has then been sought through machine learning, comparing accuracy performance from 10 machine learning models. 

The global results are known: accuracy has sprung from 50 % with the baseline model to 88 % with eXtreme Gradient Boosting. But contribution to maximizing accuracy has not been broken down per layer, i.e. NLP, text mining and machine learning. 

<br>

### B. Breaking down Accuracy Improvement per Methodological Layer

<br>

On the validation set, the baseline delivers an accuracy level of 50 % (just as on the training set). On the validation set again, the final model provides accuracy of 88 %, which is substantially higher. 

Where does the improvement come from? From wich step in the whole process: NLP, text mining or machine learning? Splitting contribution to results needs to be done on the validation set since results on the training set can be boosted by overfitting. To evaluate input from each layer, a simple approach will we followed.

On the validation set without polarization and without negation, prediction will be conducted and accuracy produced. The method will be CART with 15-value tuning and 25 bootstrapped resamples: this model is rather fast and can be considered as appropriate without being "machine learning optimization". This will help evaluate the impact of NLP. 

Then, on the validation set with negation, a second evaluation will be conducted: this will help evaluate the impact of integrating negation. 

On the validation set with negation and polarization, a third evaluation will be conducted: this will help evaluate the impact of polarization. 

The accuracy difference between this last result and the result from eXtreme Gradient Boosting will measure the impact of machine learning optimization. 

<br>

```{r Running CART to disentangle impact from NLP and text mining and ML optimization}
## FIRST: IMPACT FROM NLP

# Creating and preprocessing validation corpus.
corpus <- VCorpus(VectorSource(reviews_val$text)) 
corpus <- tm_map(corpus, content_transformer(tolower))

# Replacing all punctuation marks other than apostrophes with white space 
# characters, instead of simply suppressing punctuation marks, not to risk 
# collapsing two or more words into one. Keeping apostrophes 
# to leave intact short forms such as "don't" and be able to remove them.
for (i in 1:nrow(reviews_val)) {
  corpus[[i]]$content <- gsub("(?!')[[:punct:]]", " ", 
                              corpus[[i]]$content, perl = TRUE)
}
rm(i)

# Removing short forms after removing extra white space characters (all 
# white spaces except one of them in a sequence of white space characters). 
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, short_forms_neg)
corpus <- tm_map(corpus, removeWords, short_forms_pos)

# Replacing all remaining apostrophes with white space characters (there might
# be other apostrophes than in short forms...). Saving under more specific name
# to further use it. 
for (i in 1:nrow(reviews_val)) {
  corpus[[i]]$content <- gsub("[[:punct:]]", " ", corpus[[i]]$content)
}
rm(i)
corpus_2 <- corpus

# Removing n-grams from other files. 
corpus <- tm_map(corpus, removeWords, negation)
corpus <- tm_map(corpus, removeWords, stopwords_remaining)

# Stemming words.
corpus <- tm_map(corpus, stemDocument)

# Removing numbers and extra white space characters.
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# No sparsity management: validation columns will be matched with 
# training columns. 

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(dtm)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))

# Adding dependent variable
sentSparse <- sentSparse %>% 
  mutate(sentiment = reviews$sentiment) %>% as.data.frame()

# Naming validation set and reinstating former training set.
val <- sentSparse
train <- sentSparse_av0

# For machine learning, columns have to match between training set 
# and test set: adjustments have to be made on the validation set.
# Let's keep only colums that alo exist in the training set. 
# The column "sentiment" will remain since the name exists in both sets.
val <- val %>% as.data.frame() %>% 
               select(intersect(colnames(.), colnames(train)))

# Columns from train that are missing in val have to be added as null vectors.
mis <- setdiff(colnames(train), colnames(val))
df <- data.frame(matrix((nrow(val) * length(mis)), 
                        nrow = nrow(val), ncol = length(mis)) * 0) %>%
      `colnames<-`(mis)
val <- cbind(val, df) %>% as.data.frame()

# Training CART with the algorithm rpart with cp tuning.
set.seed(1)
fit <- train(sentiment ~ .,
             method = "rpart",
             data = train,
             tuneLength = 15,
             metric = "Accuracy")
pred <- predict(fit, newdata = val)
acc_val_cart_15_NLP <- round(mean(pred == val$sentiment), 4)          

#######################################################

## SECOND: IMPACT FROM TEXT MINING: INTEGRATING NEGATION

# The first part is common. Let's recuperate corpus_2.
corpus <- corpus_2

# Removing stopwords_remaining, stemming, removing numbers, 
# digits and multiple white space characters (leaving only
# one white space character at a time).
corpus <- tm_map(corpus, removeWords, stopwords_remaining)
corpus <- tm_map(corpus, stemDocument)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)

# Building bag of words, converting to data frame, regularizing 
# column names and adding dependent variable. No sparsity management 
# since columns will be adjusted on training set columns.
dtm <- DocumentTermMatrix(corpus)
sentSparse <- as.data.frame(as.matrix(dtm)) 
colnames(sentSparse) <- make.names(colnames(sentSparse))
sentSparse <- sentSparse %>%  mutate(sentiment = reviews$sentiment)

# Naming validation set and reinstating previous training set.
val <- sentSparse
train <- sentSparse_av1_a

# For machine learning, columns have to match between training set 
# and test set: adjustments have to be made on the validation set.
# Let's keep only colums that alo exist in the training set. 
# The column "sentiment" will remain since the name exists in both sets.
val <- val %>% as.data.frame() %>% 
               select(intersect(colnames(.), colnames(train)))

# Columns from train that are missing in val have to be added as null vectors.
mis <- setdiff(colnames(train), colnames(val))
df <- data.frame(matrix((nrow(val) * length(mis)), 
                        nrow = nrow(val), ncol = length(mis)) * 0) %>%
      `colnames<-`(mis)
val <- cbind(val, df) %>% as.data.frame()

# Training CART with the algorithm rpart with cp tuning.
set.seed(1)
fit <- train(sentiment ~ .,
             method = "rpart",
             data = train,
             tuneLength = 15,
             metric = "Accuracy")
pred <- predict(fit, newdata = val)
acc_val_cart_15_NLP_negation <- round(mean(pred == val$sentiment), 4)

accs <- c(acc_baseline, acc_val_cart_15_NLP, 
          acc_val_cart_15_NLP_negation, acc_cart_15, acc_xgb)
meth <- c("Baseline Model",
          "NLP + CART + Extra Tuning",
          "NLP + Negation + CART + Extra Tuning",
          "NLP + Negation + Polarization + CART + Extra Tuning",
          "NLP + Negation + Polarization + XGBoost + Default Tuning")
df <- data.frame(meth = meth, accs = accs, stringsAsFactors = FALSE) %>%
      `colnames<-`(c("METHODOLOGY", "ACCURACY ON THE VALIDATION SET"))

kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "#808080") %>%
  row_spec(2:4, bold = T, color = "white", background = "#007ba7") %>%
  row_spec(5, bold = T, color = "white", background = "#50c878") 

rm(reviews, short_forms_neg, short_forms_pos, negation, stopwords_remaining)
rm(corpus, corpus_2, dtm, sparse, sentSparse)
rm(ind_train, ind_val, train, val, fit, pred)
rm(accs, acc_baseline, acc_val_cart_15_NLP, 
   acc_val_cart_15_NLP_negation, acc_cart_15, acc_xgb)
rm(meth, df)
```

## VI. SUMMARY & CONCLUSION

<br>

Dear Readers, 

As a conclusion, may I suggest reading the Executive Summary again, which is in front of this document?

<br>

## VII. REFERENCES

<br>

Availability has been checked up on March 30, 2020.

<br>

### A. Sentiment Analysis

<br>

https://www.edx.org/course/the-analytics-edge 

https://www.tidytextmining.com/sentiment.html 

https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb

https://cran.r-project.org/web/packages/SentimentAnalysis/vignettes/SentimentAnalysis.html

https://monkeylearn.com/sentiment-analysis/

https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386

https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c

<br>

### B. Text Mining

<br>

https://www.tidytextmining.com/tidytext.html

https://monkeylearn.com/text-mining/

<br>

### C. About Resample Distribution of Accuracy

<br>

https://books.google.be/books?id=GgmqDwAAQBAJ&pg=PA80&lpg=PA80&dq=in+train()+evaluating+accuracy+standard+error+with+results$accuracy+or+with+resample$accuracy&source=bl&ots=faQVigVhBJ&sig=ACfU3U0xXniRHQQGQr_uc88JVW5mCvPSYA&hl=en&sa=X&ved=2ahUKEwjumcXH-bjoAhVBC-wKHc2qBikQ6AEwCnoECAcQAQ#v=onepage&q=in%20train()%20evaluating%20accuracy%20standard%20error%20with%20results%24accuracy%20or%20with%20resample%24accuracy&f=false

https://www.edx.org/course/data-science-machine-learning

<br>

## D. Something Simple about Overfitting

<br>

https://www.r-bloggers.com/machine-learning-explained-overfitting/

# *********************************************************************************
# *********************************************************************************
