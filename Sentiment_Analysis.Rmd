---
title: "Prediction in Sentiment Analysis - Amazon Sample"
subtitle: "Final Report"
author: "Philippe Lambot"
date: "March 31, 2020"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r Setup, include = FALSE}

# In the YAML, I have asked a TOC (table of contents). 
# I have also chosen to produce an html_document and issue it in PDF format. 

# In the opts_chunk just below, I have chosen options to avoid messages and warnings in hmeq_Final_Report.html. Messages and warnings produced by the code have already been dealt with.  
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# The next opts_chunk regulates figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# The next instruction facilitates table layout in HTML.
options(knitr.table.format = "html")

# After the present chunk, there are 13 lines of code to further regulate layout:
# - the 1st block prevents bullets appearing in the TOC (Table of Contents);
# - the 2nd block determines font size in body text parts;
# - the 3rd block generates text justification.

# Last about layout, I use the string <br> to generate empty lines.
```

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<font size="3">

<style>
body {
text-align: justify}
</style>

## <br>
## <br>
## <br>

## *********************************************************************************

## Prediction in Sentiment Analysis - Amazon Sample

## ********************************************************************************* 

<br>

## Executive Summary

<br>

This is a sentiment analysis about an Amazon data sample provided in the UCI Machine Learning Repository. One thousand reviews have received sentiment orientation, i.e. "positive" or "negative". The challenge in this project is maximize accuracy on predicting sentiment orientation on a validation set of one third. Accuracy is representative since prevalence of positive sentiment orientation is 50 %. An accuracy level of 84 % has been  reached on the validation set at the end of a three-tier analysis.

First, Natural Language Processing has been conducted in terms of lowercasing, punctuation removal, stemming, tokenization and bag of words, followed by a first CART prediction on the training set. Some fine tuning proved necessary to cope with short forms (intra word contractions), punctuation marks stuck to words, alternative grammar, etc. 

Second, text mining has focused on word frequency, wordclouds, decision trees and  analysis of false positives and of false negatives, which have been pinpointed as a weak point. Text ming has opened up two avenues for improvement: reintegrating negational unigrams ("not", etc.) and text classification. Text classification applied to unigrams or multigrams conveying some subjective information that were present in false negatives or positives but didn't show in decision trees; they have been classified as negative of positive sentiment orientation and replaced with one generic negative sentiment token and one generic positive sentiment token. Running CART again propelled accuracy to higher levels. 

Third, machine learning models were then compared in accuracy and, complementarily, in other performance metrics. Three out of ten have been picked up: svmRadialCost, which delivered the highest accuracy level, rf and xbgBoost, which produced the highest specificity. To keep on the safe line, an ensemble model has been built up by majority vote. The ensemble model has produced an accuracy level of 87 %, to be compared with 50 % for the basiline model.

<br>

TAGS: sentiment analysis, natural language processing, text mining, subjective information, tokenization, bag of words, word frequency, wordcloud, decision trees, CART, false negatives, false positives, text classification, polarization, lists of positive n-grams, lists of negative n-grams, machine learning, classification,  eXtreme Gradient Boosting, Random Forests, Stochastic Gradient Boosting, AdaBoost Classification Trees, Support Vector Machines with Radial Basis Function Kernel, Monotone Multi-Layer Perceptron Neural Network, bootstrapping, resample distribution of accuracy, ensemble model, R

<br>

GitHub: https://github.com/Dev-P-L/Sentiment-Analysis

<br>

## Foreword to Readers

<br>

Dear Readers, you are most welcome to run the project on your own computer if you so wish.

This project is lodged with the GitHub repository https://github.com/Dev-P-L/Sentiment-Analysis.

It is comprised of eleven files. All code is included in hmeq_Script.Rmd. It does not show in hmeq_Final_Report.html. 

For your convenience, the dataset has already been downloaded onto my GitHub repository 
https://github.com/Dev-P-L/Home-Equity-Loan-Default-Prediction wherefrom it will be automatically retrieved by hmeq_Script.Rmd code. If you so wish, you can also
easily retrieve the dataset from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences and adapt hmeq_Script.Rmd code accordingly.

You can knit hmeq_Script.Rmd (please in HTML) and produce hmeq_Final_Report.html on your own computer. On my laptop, running hmeq_Script.Rmd takes approximately four hours. For information, here are some characteristics of my environment:

 - R version 3.5.1 (2018-07-02) -- "Feather Spray",
 - RStudio Version 1.1.456 - ? 2009-2018,
 - Windows 10.

Some packages are required by hmeq_Script.Rmd. The code from hmeq_Script.Rmd contains instructions to download these packages if they are not available yet. 

<br>

```{r Cleaning up workspace and downloading packages}

# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(tm)) install.packages("tm", repos = "http://cran.us.r-project.org")
if(!require(SnowballC)) install.packages("SnowballC", repos = "http://cran.us.r-project.org")
if(!require(wordcloud)) install.packages("wordcloud", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(kernlab)) install.packages("kernlab", repos = "http://cran.us.r-project.org")
if(!require(fastAdaboost)) install.packages("fastAdaboost", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(monmlp)) install.packages("monmlp", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(utf8)) install.packages("utf8", repos = "http://cran.us.r-project.org")

# Requiring libraries.
library(tidyverse)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)
library(kernlab)
library(fastAdaboost)
library(randomForest)
library(gbm)
library(xgboost)
library(monmlp)
library(kableExtra)
library(gridExtra)
library(utf8)
```

Let's get in touch with data. 

<br>

## I. Getting in touch with Data

<br>

According to the UCI website, data are organized in a CSV file in two columns: in the first column, there are single sentences that are reviews of products; in the second column, there is a positive or negative evaluation. The ratio of positive evaluations is 50 %.

Actually, there are three files. The first will be dowloaded. It is a sample of 1,000 Amazon reviews. 

That file will be split into a training set, with two thirds of reviews, and a validation set. Let's have a first quick look at a few reviews from the training set.

<br>

```{r Dowloading data}
myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/amazon_cells_labelled.txt"
reviews <- read.delim(myfile, header = FALSE, sep = "\t", quote = "", 
                      stringsAsFactors = FALSE)
rm(myfile)

reviews <- reviews %>% 
  `colnames<-`(c("text", "sentiment")) %>%
  mutate(sentiment = as.factor(gsub("1", "Appreciating", 
         gsub("0", "Critisizing", sentiment)))) %>% as.data.frame()

# Creating training index and validation index.
set.seed(1)
ind_train <- createDataPartition(y = reviews$sentiment, 
                                 times = 1, p = 2/3, list = FALSE)
ind_val <- as.integer(setdiff(1:nrow(reviews), ind_train))

# ind_train allows to select the reviews that will be used for training, 
# be it in natural language processing, in text mining or in 
# machine learning.

# Selecting a few reviews at random from the training reviews.
sample_size <- 12
set.seed(1)
seq <- sample(ind_train, sample_size, replace = FALSE)

tab <- reviews[seq, ] %>% as.data.frame() %>% 
  `colnames<-`(c("TRAINING REVIEW", "SENTIMENT")) %>% `rownames<-`(seq)
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1:sample_size, bold = T, color = "white", background = "blue")
rm(sample_size, seq, tab)
```

<br>

After shaking hands with data, let's proceed to some NLP.

<br>

## II. Natural Language Processing

### A. Corpus - Tokenization - Bag of Words

<br>
 
After data being downloaded, they will be reorganized into a corpus. Then they will be processed in NLP: words will be lowercased, punctuation marks will be removed as well as stopwords and finally words will be stemmed. 

Tokenization will then take place, a bag of words being created. Applying a sparsity threshold will only leave tokens that appear in at least 0.5 % of reviews. The bag of words takes the form of a Document Term Matrix, where the 1,000 rows correspond to the 1,000 reviews and there is a column for each token selected across the sparsity process. At the junction of each row and each column, there is a frequency number representing the number of occurrences of the corresponding token in the corresponding review. 

As a pre-attentive took, a wordcloud will show the most frequent tokens and that will be our first visual contact with data. 

```{r Adapting opts_chunk to prepare for a wordcloud}
# Occupying 100% out of printable width instead of 60% since a wordcloud needs some 
# space to expand and be readily readable. 
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Creating corpus and bag of words}
# NATURAL LANGUAGE PROCESSING 
# Creating corpus.
# Corpus is created on training reviews only to avoid any interference
# between training reviews and validation reviews. Otherwise, 
# validation token frequencies could slightly impact token selection
# when applying the sparsity threshold. 
reviews_train <- reviews[ind_train, ]
corpus <- VCorpus(VectorSource(reviews_train$text)) 

# Lowercasing, removing punctuation and stopwords, stemming document.
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)

# Building up a bag of words in a Document Term Matrix.
dtm <- DocumentTermMatrix(corpus)

# Managing sparsity with sparsity threshold. 
sparse <- removeSparseTerms(dtm, 0.995)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse <- as.data.frame(as.matrix(sparse)) 

# Making all column names R-friendly.
colnames(sentSparse) <- make.names(colnames(sentSparse))

# The results from the NLP process need checking. 
# Let's build up a wordcloud with the most frequent tokens.
set.seed(1)
wordcloud(colnames(sentSparse), colSums(sentSparse), min.freq = 10, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Readapting opts_chunk to go back to 60% width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

Some tokens were not expected, such as "dont" or "ive" since they seem to originate in short forms and were expected to have been eliminated by stopwords being removed. Let's start with "dont". How many rows contain " dont "?

<br>

### B. Checking up NLP Output

<br>

```{r Investigating "dont"}
# Which rows from the training reviews have a 1 in the column "dont"? 
# This would mean that the corresponding NLP-transformed reviews contain "dont".
bin <- which(sentSparse$dont == 1)

# How many rows contain " dont "?
df <- data.frame(length(bin)) %>% 
  `colnames<-`('OCCURENCES OF "dont" IN BAG OF WORDS') 
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue") 
rm(df)
```

<br>

Let's have a look at the first review that has produced "dont".

<br>

```{r First review producing "dont"}
df <- data.frame(reviews_train$text[bin[1]]) %>%
  `colnames<-`('FIRST TRAINING SET REVIEW GENERATING "dont"') 
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue") 
rm(df)
```

<br>

"dont" contains a spelling error or is "alternative" grammar. Nevertheless, ideally, it should be treated as a short form from standard grammar, i.e. as the short form "don't". Consequently, if we want to eradicate short forms, we'll have to complement
stopwords with variants such as "dont", "couldnt". This will be done under the form of an additional stopword file called "extra_stopwords.csv". 

Let's see the second review that, after NLP, has generated "dont".

<br>

```{r Second review generating "dont"}
df <- data.frame(reviews_train$text[bin[2]]) %>% 
  `colnames<-`('SECOND TRAINING SET REVIEW GENERATING "dont"') 
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue") 
rm(bin, df)
```

<br>

This is another scenario: "don't" has been written in the standard way, but all punctuation marks have been removed, consequently it has become "dont" and is no longer identical to the stopword "don't" and will not be removed. Consequently, stopwords containing apostrophes should be removed before removing punctuation. Or punctuation marks should be removed except for apostrophes and hyphens by using parameters of the function removePunctuation() or other coding. A solution will be applied. 

Let's now analyze all tokens emanating from reviews_train, not just the most frequent ones. For brevity, only impactful results will be showcased. There are several unigrams that seem to originate from bigrams, e.g. "brokeni" at row 24.

<br>

```{r Row of tokens including "brokeni"}
# Let's prepare a presentation table. Collecting all tokens.
tokens <- findFreqTerms(dtm, lowfreq = 1)

# Defining the number of columns of the presentation table. 
nc <- 5           

# Calculating the number of missing values to get a full matrix
# and adding hyphens accordingly to get a full matrix. 
mis <- ((ceiling(length(tokens) / nc)) * nc) - length(tokens)
tokens <- as.character(c(tokens, rep("-", mis)))
tokens <- data.frame(matrix(tokens, ncol = nc, byrow = TRUE)) %>%
  `colnames<-`(NULL) 
rm(nc, mis)

# Printing the row containing "brokeni".
knitr::kable(tokens[24, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue") 
```

<br>

Where does "brokeni" come from?

<br>

```{r Untransformed review generating "brokeni"}
v <- 1:nrow(reviews_train)
string <- "brokeni"
for(i in 1:nrow(reviews_train)) {
  v[i] <- length(grep(string, corpus[[i]]$content))
}

df <- data.frame(reviews_train$text[which(v == 1)], stringsAsFactors = FALSE) %>%
  `colnames<-`('TRAINING SET REVIEW PRODUCING "brokeni"') 
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue") 
rm(v, string, i, df)
```

<br>

What happened? Well, "broken...I" was first lowercased to "broken...i", then punctuation was removed by the function removePunctuation, which does not insert any 
white space character, and "broken...i" has become "brokeni". 

In further text mining and machine learning steps, "brokeni" will be treated differently than "broken", which can be seen on the same row, i.e. on row 24 of the tokens from the training reviews.

This has to be changed: instead of the function removePunctuation(), specific and different for loops will be developped, replacing punctuation marks with white space characters instead of just removing punctuation marks and allowing for differentiated treatment of on the one hand apostrophes and on the other hand other punctuation marks.

A result similar to "brokeni" can be seen on row 1: it is "abovepretti".

```{r Showing "abovepretti"}
knitr::kable(tokens[3, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue") 
```

<br>

Where does "abovepretti" come from?

<br>

```{r Untransformed review generating "abovepretti"}
v <- 1:nrow(reviews_train)
string <- "abovepretti"

for(i in 1:nrow(reviews_train)) {
  v[i] <- length(grep(string, corpus[[i]]$content))
}

df <- data.frame(reviews_train$text[which(v == 1)]) %>%
  `colnames<-`('TRAINING SET REVIEW PRODUCING "abovepretti"')
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue") 
rm(tokens, v, i, string, df)
rm(corpus, dtm, sparse, sentSparse)
```

<br>

Therefore, the preprocessing process will be rerun with for loops that replace punctuation marks with white space characters and allowing for differentiated treatment of on the one hand apostrophes and hyphens (dashes) and on the other hand other punctuation marks.

Instead of one stopword list generated by the function stopword("english"), we'll split it into four files while complementing a little bit the list. This 

Stopwords will be split into stopwords with apostro
phe, stopwords without apostrophe, negational stopwords and extra stopwords (short forms written without apostrophe).

<br>

### C. Fine Tuning NLP

<br>

Buildind up extra stopwords file: 19 extra stopwords, which are short forms without apostrophe like "isnt". With this file, short forms such as "isnt" can be removed. 

Moreover, stopwords have been split into 3 files: 

- stopwords_with_apostrophe.csv,
- stopwords_without_apostrophe.csv
- and stopwords_negation.csv.

The 4 files have been uploaded to my GitHub repository, https://github.com/Dev-P-L/Sentiment-Analysis. They are going to be downloaded now and integrated into the NLP transformation.

Let's rebuild the corpus, the bag of words and the wordcloud. 

```{r Adapting opts_chunk again to prepare for a wordcloud}
# Occupying 100% out of printable width instead of 60% since a wordcloud needs some 
# space to expand and be readily readable. 
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Complementing NLP}
myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/stopwords_with_apostrophe.csv"
stopwords_with_apostrophe <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
stopwords_with_apostrophe <- stopwords_with_apostrophe[, 2] %>% as.vector()
# Converting possible curly apostrophes to straight apostrophes. 
stopwords_with_apostrophe <- sapply(stopwords_with_apostrophe, utf8_normalize, map_quote = TRUE)

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/stopwords_without_apostrophe.csv"
stopwords_without_apostrophe <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
stopwords_without_apostrophe <- stopwords_without_apostrophe[, 2] %>% as.vector()

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/stopwords_negation.csv"
stopwords_negation <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
stopwords_negation <- stopwords_negation[, 2] %>% as.vector()

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Sentiment-Analysis/master/extra_stopwords.csv"
extra_stopwords <- read.csv(myfile, header = FALSE, stringsAsFactors = FALSE) 
extra_stopwords <- extra_stopwords[, 2] %>% as.vector()
rm(myfile)

# Creating and preprocessing corpus.
corpus_av0 <- VCorpus(VectorSource(reviews_train$text)) 
corpus_av0 <- tm_map(corpus_av0, content_transformer(tolower))

# Replacing all punctuation marks other than apostrophe
# with white space characters,
# instead of simply suppressing punctuation marks, 
# to prevent proces from generating tokens like "brokeni".
# Keeping apostrophes to leave intact short forms such as "don't".
for (i in 1:nrow(reviews_train)) {
  corpus_av0[[i]]$content <- gsub("(?!')[[:punct:]]", " ", 
                              corpus_av0[[i]]$content, perl = TRUE)
}
rm(i)

# Removing stopwords containing one apostrophe. 
corpus_av0 <- tm_map(corpus_av0, removeWords, stopwords_with_apostrophe)

# Replacing all remaining apostrophes with white space characters (there might
# be other apostrophes than in short forms...). 
for (i in 1:nrow(reviews_train)) {
  corpus_av0[[i]]$content <- gsub("[[:punct:]]", " ", corpus_av0[[i]]$content)
}
rm(i)

# Removing stopwords from other files. 
corpus_av0 <- tm_map(corpus_av0, removeWords, stopwords_without_apostrophe)
corpus_av0 <- tm_map(corpus_av0, removeWords, stopwords_negation)
corpus_av0 <- tm_map(corpus_av0, removeWords, extra_stopwords)

# Stemming words.
corpus_av0 <- tm_map(corpus_av0, stemDocument)

# Removing numbers and extra white space characters (all white spaces except one
# of them in a sequence of white space characters).
corpus_av0 <- tm_map(corpus_av0, removeNumbers)
corpus_av0 <- tm_map(corpus_av0, stripWhitespace)

# Building up a bag of words in a Document Term Matrix.
dtm_av0 <- DocumentTermMatrix(corpus_av0)

# Managing sparsity with the sparsity threshold. 
sparse_av0 <- removeSparseTerms(dtm_av0, 0.995)

# Converting sparse, which is a DocumentTermMatrix, 
# to a matrix and then to a data frame.
sentSparse_av0 <- as.data.frame(as.matrix(sparse_av0)) 

# Making all column names R-friendly.
colnames(sentSparse_av0) <- make.names(colnames(sentSparse_av0))

# Let's check whether shortcomings have disappeared or not. 
# Let's build up a wordcloud with the most frequent tokens
# from the training reviews.
set.seed(1)
wordcloud(colnames(sentSparse_av0), colSums(sentSparse_av0), min.freq = 10, 
          max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
```

```{r Readapting opts_chunk again to go back to 60% width for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

In the wordcloud, there is no more token originating from short forms. Let's have a broader look at all tokens and build up a presentation table. "dont" has indeed disappeared. Let's check it up in the bag of words. 

<br>

```{r Looking at tokens after complementing NLP}
# Retrieving all tokens.
tokens <- findFreqTerms(dtm_av0, lowfreq = 1)

# Choosing the number of columns of the presentation table. 
nc <- 5

# Calculating the number of missing tokens to have a full matrix. 
mis <- ((ceiling(length(tokens) / nc)) * nc) - length(tokens)

# Builing up presentation table.
tokens <- as.character(c(tokens, (rep("-", mis))))
tokens <- data.frame(matrix(tokens, ncol = nc, byrow = TRUE)) %>%
  `colnames<-`(NULL) %>% `rownames<-`(NULL)

# Looking for "dont".
knitr::kable(tokens[51, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
rm(nc, mis)
```

<br>

So has "ive"!

<br>

```{r Showing that "ive" has disappeared}
knitr::kable(tokens[96, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
```

<br>

The same again for "brokeni".

```{r Showing that "brokeni" has disappeared}
knitr::kable(tokens[21, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
```

<br>

And "abovepretti" has also vanished.

<br>

```{r Showing that "abovepretti" has disappeared}
knitr::kable(tokens[1, ], "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
```

<br>

As well as many other oddities. I leave uncorrected some spelling errors, such as among others "disapoint" or "dissapoint" because this would require manual intervention and seems marginal. 

Let's have a first try at predicting sentiment on the basis of sentSparse_av0, which is our training set. 

<br>

### D. Measuring NLP Impact on Machine Learning Accuracy

<br>

The chosen machine learning model will be CART: it runs rather quicky and delivers clear decision trees. Running function rpart() on the training set delivers the accuracy level mentioned hereunder. 

<br>

```{r Running rpart() for the first time on the training set}
# Adding dependent variable.
sentSparse_av0 <- sentSparse_av0 %>% mutate(sentiment = reviews_train$sentiment)

# Training CART with the algorithm rpart.
set.seed(1)
fit_cart_av0 <- rpart(sentiment ~., data = sentSparse_av0)
fitted_cart_av0 <- predict(fit_cart_av0, type = "class")
cm_cart_av0 <- confusionMatrix(fitted_cart_av0, sentSparse_av0$sentiment)

# The accuracy level is 
df <- data.frame(round(cm_cart_av0$overall["Accuracy"], 4)) %>%
  `rownames<-`("Model rpart") %>% `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
rm(df)
```

<br>

Now let's train the rpart method with the train() function from the caret package. 

By default, the train() function would train across 3 values of cp, i.e. the complexity parameter, resampling method, and with a resampling method based on 25 bootstrapped resamples for each tuned value of cp. 

The default resampling method seems appropriate in this context. The size of each resample will be the same of the size of the training set, i.e. 668. Not working with a smaller number of rows matters because 668 is already a limited size. Working with e.g. k-fold cross-validation would imply further splitting the training set. By the way, let's remember that reasmpling is sampling with replacement, some reviews being picked up twice or more and some other reviews not being selected. 

As far as the number of tuned values is concerned, let's upgrade it to 15 to increase the odds of improving accuracy? the more so since rpart runs rather quickly. 

Will accuracy improve?

<br>

```{r Running rpart with train() and 15 as tuneLength}
set.seed(1)
fit_cart_tuned_av0 <- train(sentiment ~ .,
                         method = "rpart",
                         data = sentSparse_av0,
                         tuneLength = 15,
                         metric = "Accuracy")
fitted_cart_tuned_av0 <- predict(fit_cart_tuned_av0)
cm_cart_tuned_av0 <- confusionMatrix(as.factor(fitted_cart_tuned_av0), 
                                     as.factor(sentSparse_av0$sentiment))

# The tuned rpart model delivers an accuracy level of
df <- data.frame(round(cm_cart_tuned_av0$overall["Accuracy"], 4)) %>%
  `rownames<-`("CART Model with cp Tuning") %>% 
  `colnames<-`("ACCURACY ON THE TRAINING SET")
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
rm(df)
```

<br>

Accuracy increases somewhat. Consequently, this method will be further used as a yardstick when further moving into NLP and text mining. 

For the record, let's have a look at a graph showing how accuracy evolves across the 15 cp values chosen by the train() function. 

<br>

```{r Graph of accuracy across cp values on resamples}
graph <-  
  ggplot(fit_cart_tuned_av0) + 
  geom_line(col = "blue", size = 1) +
  geom_point(col = "blue", size = 4) +
  ggtitle("Average Bootstrap Accuracy across cp Values") +
  xlab("Complexity Parameter") + ylab("Average Accuracy (Bootstrap)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), 
        axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(graph)
```

<br>

On the graph above, maximum accuracy is a bit different than the than the level previously indicated, actually a bit lower. Why is it different? Because on the graph it is, for each cp value, the average accuracy on the 25 bootstrapped resamples while accuracy previously given related to the whole training set.

The optimal value of cp is near zero. Is it really zero? 

<br>

```{r Optimal cp value}
df <- data.frame(round(fit_cart_tuned_av0$bestTune, 4)) %>%
  `colnames<-`("OPTIMAL CP VALUE FROM CART") 
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
rm(df)
```

<br>

Yes, it is. This means that the train() function has kept the decision tree as complex as possible by assigning a zero value to the complexity parameter. 

On the whole training set, the rpart model delivers 78 % accuracy and the rpart model with tuning gives 79 %. Both levels are substantially higher than accuracy provided by the baseline model. The baseline model would predict a positive evaluation for all reviews (or alternatively a negative evaluation for all reviews). What accuracy level would the baseline model deliver? 

<br>

```{r Accuracy from baseline model}
df <- sentSparse_av0
pred_baseline <- 
  data.frame(sentiment = rep("Appreciating", nrow(df))) %>%
  mutate(sentiment = factor(sentiment, levels = levels(df$sentiment)))
cm_baseline <- confusionMatrix(pred_baseline$sentiment, 
                               as.factor(df$sentiment)) 
df <- data.frame(round(cm_baseline$overall["Accuracy"], 4)) %>%
      `colnames<-`("ACCURACY FROM BASELINE MODEL ON TRAINING SET") %>%
      `rownames<-`(NULL)
knitr::kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, color = "white", background = "blue")
rm(df)
```

<br>

Let's summarize results from the three models, not only with accuracy but also with additional performance metrics. 

<br>

```{r Summary table}
colname <- c("MODEL ID", "SHORT DESCRIPTION", "ACCURACY", "SENSITIVITY", 
             "NEG PRED VAL", "SPECIFICITY", "POS PRED VAL")
models <- c("baseline", "cart_av0", "cart_tuned_av0")
description <- c("baseline model", "rpart", "rpart + cp tuning")
cm <- c("cm_baseline", "cm_cart_av0", "cm_cart_tuned_av0")
tab <- data.frame(matrix(1:(length(colname) * length(models)),
                         ncol = length(colname), nrow = length(models)) * 1)

for (i in 1:length(models)) {
  tab[i, 1] <- models[i]
  tab[i, 2] <- description[i]
  tab[i, 3] <- 
    eval(parse(text = paste(cm[i], "$overall['Accuracy']", sep = "")))
  tab[i, 4] <- 
    eval(parse(text = paste(cm[i], "$byClass['Sensitivity']", sep = "")))
  tab[i, 5] <- 
    eval(parse(text = paste(cm[i], "$byClass['Neg Pred Value']", sep = "")))
  tab[i, 6] <- 
    eval(parse(text = paste(cm[i], "$byClass['Specificity']", sep = "")))
  tab[i, 7] <- 
    eval(parse(text = paste(cm[i], "$byClass['Pos Pred Value']", sep = "")))
}                 
                  
tab_av0 <- tab %>% mutate_at(vars(3:7), funs(round(., 4))) %>%
           `colnames<-`(colname)
knitr::kable(tab_av0, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  row_spec(1, bold = T, strikeout = T, 
           color = "#9bc4e2", background = "blue") %>%
  row_spec(2, bold = T, color = "white", background = "blue") %>%
  row_spec(3, bold = T, color = "white", background = "green")
rm(cm_baseline, pred_baseline, models, description, cm, tab)
```

<br>

In the table above, on row 1, fonts have been blurred into light blue and have been stricken through to indicate that this model has been discarded. The other two models should be seen as a cumulative process bringing accuracy improvement in a stepwise and incremental way, with the one on green background being the best in accuracy.

Accuracy with the baseline model is 50 %, which reflects prevalence. Models 2 and 3 are sensibly higher in accuracy. Model 2 gives 78 % and model 3 79 % in accuracy. 

With model 2 or 3, sensitivity and negative predictive value are lower than specificity and positive predictive value. This reflects false negatives being more numerous than false positives. False negatives are predictions pointing to "Critisizing" while the reference value is "Appreciating". This is an insight for text mining: perusing the false negatives and coming with some improvement. Imbalance is a little reduced with model 3 in comparison with model 2.

In order to confirm that false negatives are more numerous than false positives, let's have a look at the confusion matrix for both models. First, the confusion matrix from the rpart model without tuning. 

<br>

```{r Confusion matrix for the rpart model without tuning}
name <- c("TP = ", "FN = ", "FP = ", "TN = ")
tab <- table(fitted_cart_av0, sentSparse_av0$sentiment) %>% 
  as.vector() %>% paste(name, ., sep = "")
tab <- data.frame(matrix(tab, ncol = 2, nrow = 2, byrow = FALSE)) %>%
  `colnames<-`(c("Appreciating (ref)", "Criticizing (ref)")) %>%
  `rownames<-`(c("Appreciating (pred)", "Criticizing (pred)"))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "black") %>%
  column_spec(2, bold = T, color = "white", background = "magenta") %>%
  column_spec(3, bold = T, color = "white", background = "blue")
rm(tab)
```

<br>

The weak point lies in the first column, on magenta background: the relatively high number of false negatives and, as a corollary, the relatively low number of true positives. On the reference positive class ("Appreciating" in label), predicting  seems problematic or at the very least challenging: false negatives are rife. On the contrary, on the reference negative class ("Criticizing" in label), predicting  has run smoothely, with a satisfactorily low number of false positives. 

The tuned rpart model is expected to to slightly reduced the excess in false negatives.

<br>

```{r Confusion matrix for the rpart model with tuning}
name <- c("TP = ", "FN = ", "FP = ", "TN = ")
tab <- table(fitted_cart_tuned_av0, sentSparse_av0$sentiment) %>% 
  as.vector() %>% paste(name, ., sep = "")
tab <- data.frame(matrix(tab, ncol = 2, nrow = 2, byrow = FALSE)) %>%
  `colnames<-`(c("Appreciating (ref)", "Criticizing (ref)")) %>%
  `rownames<-`(c("Appreciating (pred)", "Criticizing (pred)"))
knitr::kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", 
                full_width = F, font_size = 16) %>%
  column_spec(1, bold = T, color = "black") %>%
  column_spec(2, bold = T, color = "white", background = "magenta") %>%
  column_spec(3, bold = T, color = "white", background = "blue")
rm(tab)
rm(cm_cart_av0, fit_cart_av0, fitted_cart_av0)
```

<br>

Wit the tuned rpart model, globally accuracy has slightly improved: the sum of numbers on the main diagonal is larger. 

On the magenta background figure, predicting on the reference positive class is less prolific in false negatives and, as a corollary, true positives are more predominant. 

On the secondary diagonal, imbalance between false negatives and false positives is less marked, not only because false negatives are less numerous but also because false positives are a bit more numerous. Nevertheless false negatives remain the weak point, being twice as numerous as false positives. 

False negatives - and false positives - will be perused through text mining in the next section, looking for new insights towards accuracy improvement. 

<br>

## III. Text Mining towards Higher Accuracy

<br>

Let's now have a look at the false negatives from the CART model with tuning, which constitute a challenge.

<br>

### A. Analysis of False Negatives

#### 1. Visualizing False Negatives

<br>

```{r False negatives from cart_tuned}
df <- sentSparse_av0 %>% mutate(pred = fitted_cart_tuned_av0) %>% as.data.frame()
FN_train <- ifelse(df$sentiment == "Appreciating", 1, 0) - 
  ifelse(df$pred == "Appreciating", 1, 0)
FN_train <- ifelse(FN_train == 1, 1, 0)

# Let's build up a sample taken out of all false negatives. 
sample_size <- 12
set.seed(1)
seq <- sort(sample(which(FN_train == 1), sample_size, replace = FALSE))
rm(FN_train)

# Let's save seq under the name seq_FN for further use. 
seq_FN <- seq

# Let's build up a presentation table.
df <- data.frame(matrix(nrow = sample_size, ncol = 2) * 1)

for (i in 1:length(seq)) {
  row <- as.numeric(seq[i])
  df[i, 1] <- reviews_train[row, 1]
  df[i, 2] <- corpus_av0[[row]]$content
}
rm(seq, sample_size, i, row)

colname_token <- c("REVIEW", "PRE-PROCESSING", "USABLE SUBJECTIVE INFORMATION")
comment <- c("super", "fast + faster", "prettier + sharp", "infatu", "wise",
             "awesom", "rock", "troubl", "?", "?", "secur", "quick")
df <- df %>% mutate(com = comment) %>% `colnames<-`(colname_token)
rm(comment)

# Let's save df under a less anonymous name for further use and print it. 
df_FN_cart <- df
rm(df)
df_FN_cart <- kable(df_FN_cart, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>%
  row_spec(1:7, bold = T, color = "white", background = "blue") %>%
  row_spec(8, bold = T, color = "white", background = "purple") %>%
  row_spec(9:10, bold = T, color = "white", background = "magenta") %>%
  row_spec(11:12, bold = T, color = "white", background = "blue")
df_FN_cart
```

<br>

There are three scenarios, each illustrated with a different background color.

<br>

#### 2. Subjective Information Unigrams

<br>

On the blue background in the table above, there 9 cases out of 12. In each review, there is at least one token with subjective information that is clear from a human point of view. Let's just pinpoint "super", "fast", "faster", "prettier", "infatuated", "wise", "awesome", "rocks", etc., or, after stemming "super", "fast", "faster", "prettier", "infatu", "wise", "awesom", "rock", etc. 

These words, and the related standardized tokens, can be classified in two categories: - compliance-related tokens, expressing compliance or incompliance with 
expectations, requirements or advertisements ("fast", "faster", "rocks", etc.),
- sentiment-related tokens other than in previous category ("infatuated").

The difference between the two categories can sometimes be tricky and both categories
will be referred to altogether in this project using phrases such as "subjective information" or "tokens conveying subjective information". 

"Unfortunately", the tokens pinpointed among the false negatives do not show in the decision trees. Let's visualize the decision tree from the CART model with tuning. 

<br>

```{r Adapting opts_chunk to print a decision tree}
# Occupying 100% out of printable width instead of 60% since a wordcloud needs some 
# space to expand and be readily readable. 
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Decision tree from CART with tuning}
prp(fit_cart_tuned_av0$finalModel, uniform = TRUE, cex = 0.6, 
    box.palette = "auto")
```

```{r Readapting opts_chunk after decision tree to have 60% width again for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

<br>

What types of tokens can be seen in the decision tree? There is a majority of tokens conveying subjective information ("great", "comfort", "love", "like", disappoi", etc.). They are usually the highest ranked. There are other types of tokens:
- intent-related tokens ("purchas", "buy") or
- topic-related tokens ("plug", "ear").

Which is an interesting insight. In CART, tokens conveying subjective information predominate, which is not at all surprizing! This points to solutions allocating higher priority to tokens conveying subjective information. 

Although the majority of tokens are conveing subjective information in the decision tree, the same type tokens present among the false negatives do not show. It is probably a matter of word (or token) frequency. This can be first checked up in the wordcloud that has already been visualized. 

```{r Adapting opts_chunk to print a wordcloud}
# Occupying 100% out of printable width instead of 60% since a wordcloud needs some 
# space to expand and be readily readable. 
knitr::opts_chunk$set(out.width = "100%", fig.align = "center")
```

```{r Wordcloud}
df <- sentSparse_av0[, - ncol(sentSparse_av0)]
set.seed(1)
wordcloud(colnames(df), colSums(df), 
          min.freq = 10, max.words = 50, random.order = FALSE, rot.per = 1/3, 
          colors = brewer.pal(8, "Dark2"), scale = c(4,.5))
rm(df)
```

```{r Readapting opts_chunk to have 60% width again for graphs}
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")
```

The wordcloud, which requires a minimal frequency of 10, is not comprised of the tokens pinpointed among the false negatives. 

Among tokens depicted in the wordcloud, there are 
- topic-related tokens ("phone", "batteri", "headset", "sound", "ear", etc.),
- intent-related tokens ("purchas", "buy", "bought"),
- compliance-related tokens, expressing compliance or incompliance with 
expectations, requirements or advertisements ("fit", "comfort", "problem", etc.),
- sentiment-related tokens other than in previous category ("love", "like", etc.).

"phone", which is topic-related, is the token with the 
highest frequency. Other topic-related tokens rank among the highest 
frequencies: "batteri", "product", "headset", "sound", etc. 
"phone" has the highest frequency in the wordcloud but does not show in the decision tree. The same holds for other highly ranked topic-related tokens. 

This looks like an interesting insight. Token frequency should not be the only criterion. Actually, there should be some hierarchy between types of tokens (subjctive information, intent-related, topic-related). This does not necessarily mean a strict hierarchy starting with types and then going down to tokens; there could be intermixture, token frequency could coexist with prioritization of subjective information. 

For illustrative purposes, tokens can be visualized in decreasing order of frequency
in the graph below.

<br>

```{r Token frequency histogram}
df <- sentSparse_av0[, - ncol(sentSparse_av0)]
freq <- data.frame(to = colnames(df), fre = as.integer(colSums(df)), 
                   stringsAsFactors = FALSE) %>% 
        arrange(desc(fre)) %>% head(., 30)
graph <-  freq %>% mutate(to = reorder(to, fre)) %>%
  ggplot(aes(to, fre)) + 
  geom_bar(stat = "identity", width = 0.80, color = "#9bc4e2", fill = "#9bc4e2") + 
  coord_flip() +
  ggtitle("Token Frequency") +
  xlab("Token") + ylab("Frequency") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12), 
        axis.text.y = element_text(size = 12))
graph
rm(df, freq, graph)
```

<br>

In conclusion, it might be impactful to garner subjective information conveyed by tokens such as "super". Since CART doesn't do it, why not replace such tokens with either a generic positive sentiment token or a generic negative token? Examples of positive minded tokens can be "super", "faster", "prettier", "infatu", "awesom", etc.). This would reduce the number of tokens conveying subjective information and provide rather high frequencies for both generic tokens. 

In this project, polarity of some tokens conveying subjective information will be inserted in additional files. That is one avenue of research.

<br>

#### 3. Negational n-grams


